
%% This program consists of the files ubcthesis.dtx, ubcthesis.ins, and
%% the sample figures fig.eps and fig.fig.
%% 
%% This file may be modified and used as a base for your thesis without
%% including the licence agreement as long as the content (i.e. textual
%% body) of the file is completely rewritten. You must, however, change
%% the name of the file.
%% 
%% This file may only be distributed together with a copy of this
%% program. You may, however, distribute this program without generated
%% files such as this one.
%% 

% This Sample thesis requires \LaTeX2e
\NeedsTeXFormat{LaTeX2e}[1995/12/01]
\ProvidesFile{ubcsample.tex}[2015/05/31 v1.72 ^^J
 University of British Columbia Sample Thesis]
% This is the \documentclass[]{} command.  The mandatory argument
% specifies the "flavour" of the thesis (ubcthesis for UBC).  The
% optional arguments (in []) specify options that affect how the
% thesis is displayed.  Please see the ubcthesis documentation for
% details about the options.
\documentclass[msc,oneside]{ubcthesis}
%
% To compile this sample thesis, issue the following commands:
% latex ubcsample
% bibtex ubcsample
% latex ubcsample
% latex ubcsample
% latex ubcsample
%
% To view use xdvi (on unix systems):
% xdvi ubcsample.dvi
%
% To make a postscript file, use dvips:
% dvips -o ubcsample.ps ubcsample.dvi
%
% To view the postscript file, use ghostview or gv (on unix systems):
% gv ubcsample.ps
%
%************************************************
% Optional packages.
%
% The use of these packages is optional, but they provide various
% tools for more flexible formating.  The sample thesis uses these,
% but if you remove the example code, you should be able to exclude
% these packages.  Only standard packages have been described here;
% they should be installed with any complete LaTeX instalation, but
% if not, you can find them at the Comprehensive TeX Archive Network
% (CTAN): http://www.ctan.org/
%

\usepackage{listings}
\usepackage{amsmath}
\usepackage{ amssymb }


\usepackage{silver}

\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\input{macros_alex}
\lstset{language=silver,numbers=left, firstnumber=1, stepnumber=1,captionpos=b}
\lstMakeShortInline[language=silver,
               basicstyle=\normalsize\silverbasicstyle,
               keywordstyle=\normalsize\silverkeywordstyle,
               columns=fixed]{"}
\usepackage{amsthm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}


%******** afterpage ***************************
% This package allows you to issue commands at the end of the current
% page.  A good use for this is to use the command
% \afterpage{\clearpage} right after a figure.  This will cause the
% figure to be inserted on the page following the current one (or on
% the current page if it will fit) but will not break the page in the
% middle.
\usepackage{afterpage}

%******** float *********************************
% This package allows you to customize the style of
% "floats"---floating objects such as figures and tables.  In
% addition, it allows you to define additional floating objects which
% may be included in a list similar to that produces by \listoftables
% and \listoffigures.  Common uses include introducing floats for
% programs and other code bits in Compute Science and Chemical Schema.
\usepackage{float}

%******** tocloft *******************************
% This package allows you to customize and define custom lists such
% as a list of programs or Chemical Scheme.  Note: if you use the
% subfigure package, you must specify that you do as an option here.
% The title option uses the default formatting.  We do not use this
% here as the default formatting is acceptable.  Use the float
% package instead unless you need the extra formatting control
% provided by tocloft.
%\usepackage[subfigure, titles]{tocloft}

%******** alltt *********************************
% The alltt package allows you to include files and have them
% formatted in a verbatim fashion.  This is useful for including
% source code from an additional file.
%\usepackage{alltt}

%******** listings ******************************
% The listings package may be used to include chunks of source code
% and has facilities for pretty-printing many languages.
%\usepackage{listings}

%******** longtable *****************************
% The longtable package allows you to define tables that span
% multiple pages.
\usepackage{longtable}

%******** graphics and graphicx *****************
% This allows you to include encapsulated postscript files.  If you
% don't have this, comment the \includegraphics{} line following the
% comment "%includegraphics" later in this file.
\usepackage{graphicx}

%******** subfigure *****************************
% The subfigure package allows you to include multiple figures and
% captions within a single figure environment.
%\usepackage{subfigure}

%******** here **********************************
% The here package gives you more control over the placement of
% figures and tables.  In particular, you can specify the placement
% "H" which means "Put the figure here" rather than [h] which means
% "I would suggest that you put the figure here if you think it looks
% good."
%\usepackage{here}

%******** pdflscape ********************************
% This allows you to include landscape layout pages by using the
% |landscape| environment.  The use of |pdflscape| is preferred over
% the standard |lscape| package because it automatically rotates the
% page in the pdf file for easier reading.  (Thanks to Joseph Shea
% for pointing this out.)
\usepackage{pdflscape}

%******** natbib ********************************
% This is a very nice package for bibliographies.  It includes options
% for sorting and compressing bibliographic entries.
\usepackage[numbers,sort&compress]{natbib}

%******** psfrag ******************************
% This allows you to replace text in postscript pictures with formated
% latex text.  This allows you to use math in graph labels
% etc. Uncomment the psfrag lines following the "%psfrag" comment
% later in this file if you don't have this package.  The replacements
% will only be visible in the final postscript file: they will be
% listed in the .dvi file but not performed.
\usepackage{psfrag}

%******** hyperref *****************************
% Please read the manual:
% http://www.tug.org/applications/hyperref/manual.html
%
% This adds hyperlinks to your document: with the right viewers (later
% versions of xdvi, acrobat with pdftex, latex2html etc.) this will
% make your equation, figure, citation references etc. hyperlinks so
% that you can click on them.  Also, your table of contents will be
% able to take you to the appropriate sections.  In the viewers that
% support this, the links often appear with an underscore.  This
% underscore will not appear in printed versions.
%
% Note: if you do not use the hypertex option, then the dvips driver
% may be loaded by default.  This will cause the entries in the list
% of figures and list of tables to be on a single line because dvips
% does not deal with hyperlinks on broken lines properly.
%
% NOTE: HYPERREF is sensitive to the ORDER in which it is LOADED.
% For example, it must be loaded AFTER natbib but BEFORE newly
% defined float environments.  See the README file with the hyperref
% for some help with this.  If you have some very obscure errors, try
% first disabling hyperref.  If that fixes the problem, try various
% orderings.
%
% Note also that there is a bug with versions before 2003/11/30
% v6.74m that cause the float package to not function correctly.
% Please ensure you have a current version of this package.  A
% warning will be issued if you leave the date below but do not have
% a current version installed.
%
% Some notes on options: depending on how you build your files, you
% may need to choose the appropriate option (such as [pdftex]) for the
% backend driver (see the hyperref manual for a complete list).  Also,
% the default here is to make links from the page numbers in the table
% of contents and lists of figures etc.  There are other options:
% excluding the [linktocpage] option will make the entire text a
% hyperref, but for some backends will prevent the text from wrapping
% which can look terrible.  There is a [breaklinks=true] option that
% will be set if the backend supports (dvipdfm for example supports
% it but does not work with psfrag.)
%
% Finally, there are many options for choosing the colours of the
% links.  These will be included by default in future versions but
% you should probably consider changing some now for the electronic
% version of your thesis.
\usepackage[unicode=true,
  linktocpage,
  linkbordercolor={0.5 0.5 1},
  citebordercolor={0.5 1 0.5},
  linkcolor=blue]{hyperref}

\usepackage{cleveref}

% If you would like to compile this sample thesis without the
% hyperref package, then you will need to comment out the previous
% \usepackage command and uncomment the following command which will
% put the URL's in a typewriter font but not link them.
%\newcommand\url[1]{\texttt{#1}}

%******** setspace *******************************
% The setspace package allows you to manually set the spacing of the
% file.  UBC may require 1.5 spacing for microfilming of theses.  In
% this case you may obtain this by including this package and issuing
% one of the following commands:
%\usepackage{setspace}
%\singlespacing
%\onehalfspacing
%\doublespacing

% These commands are optional.  The defaults are shown.  You only
% need to include them if you need a different value
\institution{The University Of British Columbia}

% If you are at the Okanagan campus, then you should specify these
% instead.
%\faculty{The College of Graduate Studies}
%\institutionaddress{Okanagan}
\faculty{The Faculty of Graduate Studies}
\institutionaddress{Vancouver}

% You can issue as many of these as you have...
% \previousdegree{B.Sc., Yale-NUS College, 2021}

% You can override the option setting here.
% \degreetitle{Jack of All Trades}

% These commands are required.
\title{Supporting Automatic Decompositions of Folds in Viper}
% \subtitle{With a Subtitle}
\author{Peeranat Tokaeo}
\copyrightyear{2023}
\submitdate{\monthname\ \number\year} % The "\ " is required after
                                      % \monthname to prevent the
                                      % command from eating the space.
\program{Computer Science}

% These commands are presently not required for UBC theses as the
% advisor's name and title are not presently required anywhere.
\advisor{Dr. Alex Summers}
%\advisortitle{Professor of Physics}

% One might want to override the format of the section and chapter
% numbers.  This shows you how to do it.  Note that the current
% format is acceptable for submission to the FoGS: If you wish to modify
% these, you should check with the FoGS explicity. prior to making
% the modifications.
\renewcommand\thepart         {\Roman{part}}
\renewcommand\thechapter      {\arabic{chapter}}
\renewcommand\thesection      {\thechapter.\arabic{section}}
\renewcommand\thesubsection   {\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}
\renewcommand\theparagraph    {\thesubsubsection.\arabic{paragraph}}
\renewcommand\thesubparagraph {\theparagraph.\arabic{subparagraph}}

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}

% Here is an example of a "Program" environment defined with the
% "float" package.  The list of programs will be stored in the file
% ubcsample.lop and the numbering will start with the chapter
% number.  The style will be "ruled".
\floatstyle{ruled}
\newfloat{Program}{htbp}{lop}[chapter]

% Here is the start of the document.
\begin{document}

%% This starts numbering in Roman numerals as required for the thesis
%% style and is mandatory.
\frontmatter

%%% The order of the following components should be preserved.  The order
%%% listed here is the order currently required by FoGS:        \\
%%% Title (Mandatory)                                           \\
%%% Preface (Mandatory if any collaborator contributions)       \\
%%% Abstract (Mandatory)                                        \\
%%% List of Contents, Tables, Figures, etc. (As appropriate)    \\
%%% Acknowledgements (Optional)                                 \\
%%% Dedication (Optional)                                       \\

\maketitle                      %% Mandatory

\clearpage

%%%THESIS APPROVAL PAGE - REQUIRED AS PART OF YOUR COMPLETED THESIS

%%%%%%% UBC FoGPS Requires the following statements at the beginning of the thesis. There is a (separate) thesis approval form that has to be signed and dated by the same individuals listed here as approving the thesis. You will have to ensure that your department submits that approval form before you will be able to create an account and upload your thesis onto ciRCle. 

\clearpage

\begin{abstract}                %% Mandatory -  maximum 350 words
In progress
  
\end{abstract}

\chapter{Lay Summary}

[Required, Maximum 150 words]

This is a simple summary of your thesis, written so that members of the public will get some idea about what you have done. 

\chapter{Preface} % Mandatory if any of the conditions are met
This thesis is original, unpublished, independent work by Peeranat Tokaeo. Alex Summers was the supervisor and helped advised on the project's direction throughout.

%%% Sections and subsections etc., in the Preface should, in general
%%% not be listed in the table of contents, so use the starred form
%%% of \section etc.
% \section*{Examples}
% Note that this preface must come before the table of contents.  Note
% also that this section ``Examples'' should not be listed in the table
% of contents, so we have used the starred form: \verb|\section*{Example}|.

\tableofcontents                %% Mandatory
\listoftables                   %% Mandatory if the thesis has tables
\listoffigures                  %% Mandatory if the thesis has figures
% \listof{Program}{List of Programs} %% Optional
%% Any other lists should come here, i.e.
%% Abbreviation schemes, definitions, lists of formulae, list of
%% schemes, glossary, list of symbols etc.

\chapter{Acknowledgements}      %% Optional
Thanks everyone.

% This is the place to thank professional colleagues and people who have
% given you the most help during the course of your graduate work.

% \chapter{Dedication} %% Optional
% Dedicated to my do
% The dedication is usually quite short, and is a personal rather than
% an academic recognition.  The \emph{Dedication} does not have to be
% titled, but it must appear in the table of contents.  If you want to
% skip the chapter title but still enter it into the Table of Contents,
% use this command \verb|\chapter[Dedication]{}|.

% Note that this section is the last of the preliminary pages (with
% lowercase Roman numeral page numbers).  It must be placed
% \emph{before} the \verb|\mainmatter| command.  After that, Arabic
% numbered pages will begin.

% Any other unusual prefactory material should come here before the
% main body.

% Now regular page numbering begins.
\mainmatter

% Parts are the largest structural units, but are optional.
% \part{Thesis}

% Chapters are the next main unit.
\chapter{Motivations}
Many useful programs rely on data structures on the program heap, like arrays and lists, for data storage and manipulation. As such, to prove many interesting properties of a program, a program verifier like Viper should be capable of modelling these data structures. 

There are two main ways to describe data structures on the heap in Viper: predicates and quantified permissions. Viper predicates can represent recursive data structures, while quantified permissions allow users to write assertions flatly about a random-access data structure. Together with predicates, Viper functions can denote properties of a recursive data structure. However, no such construct exists for quantified permissions, and expressing aggregate properties on a random-access data structure is difficult. We will refer to these aggregate properties as folds from hereon. Consider the following example with arrays.

Suppose we want to model a random-access array in Viper and describe some of its properties. Firstly, Viper models mutable data as data on the heap. Each unit of data is stored at a field location, also known as a heap location. So, to express having access to an entire array "a", one may write the following:
\begin{lstlisting}
forall i: Int :: i >= 0 && i < len(a) ==> acc(a[i])
\end{lstlisting}
Here, we use an abbreviation "a[i]" to denote the heap location of array a at index i. This expression uses a feature called quantified permissions, which involves quantifying over all integer indices "i" and expressing that the program has access to the array element corresponding to each index. By design, this definition of an array with quantified permissions is flat and does not involve recursion. We can express all the indices in the array together at once, following the data structure’s random-access nature. 

However, if we try writing cumulative properties over the entire array, the proofs will still require induction. Suppose we want to express the sum of all elements in an array, starting from some arbitrary index "start" up to the array length. Then, we can write the following function:
\begin{lstlisting}
function arraySum(a: Array, start: Int) : Int
    requires forall i: Int :: i >= start && i < len(a) 
        ==> acc(a[i])
    requires start >= 0 && start <= len(a)
{
    start >= len(a) ? 0 : a[start] + arraySum(a, start + 1)
}
\end{lstlisting}
This sum function requires access to the array from the indices "start" to the array length. Note that the sum function is defined recursively on the start index. All proofs involving changes in the sum will now require induction. 

However, this is in contrast to the random-access nature of arrays. Suppose an array element has changed at index "i". To show the change to the total sum of all elements, we would need to unroll the body of "arraySum" up to index "i" in a proof by induction. 

Instead, we can ideally observe that this property is an unordered fold, and the sum could be decomposed into 
\begin{lstlisting}
sumArray(a) = sumArray_{excluding_index_i}(a) + a[i].
\end{lstlisting}
\ie, the sum of an array is the "i"th element plus the sum of all other elements. This splitting of the fold is called a decomposition

Unfortunately, this kind of reasoning is not supported automatically by native Viper. In this master's thesis, we add to Viper the support for expressing and proving these properties as folds on random-access data structures. We implement a Viper plugin that introduces a new syntax for expressing folds on any user-defined data structures and generates relevant axioms in the background. In addition, the plugin will perform decompositions on the relevant folds automatically in response to program heap changes; this allows for reasoning without recursion, like the sumArray example above. Altogether, this plugin should help the user prove fold-like properties more succinctly, without requiring recursion or induction.

% Subsections follow
\section{Related Works (Brief)}
There are a few past attempts to support reasoning about folds. In his Ph.D. thesis, Ter-Gabrielyan describes possible axiomatizations and Viper representations of folds as a way to express heap reachability properties. He introduces the idea of heap snapshots, which we utilize in our own project. However, his technique only serves as a demonstration and does not support automatic decompositions for the proofs, which we do. 

Tierry Hoermann’s bachelor’s thesis also provides specifications and encodings of folds in Viper, but similarly, they require decompositions that are not fully automatic. His axioms involve universal quantifiers that quantify over heaps, which can only be done in Boogie, a language which Viper compiles when using the Carbon verifier. 
Unfortunately, implementation details were largely omitted, and the project code on Bitbucket is no longer accessible. We are unable to reproduce his results.
	
Outside of Viper, the Spec\# program verifier provides a number of predefined fold expressions, namely sum, count, product, min and max. Each type of fold is supported by Boogie axioms inductive on integers. 
A user could potentially decompose the integer range into two sub-ranges to circumvent inductive reasoning, but this must be done manually. 
Otherwise, the user will still tend to rely on induction for the proofs involving folds. In contrast to Spec\#, our tool allows the user to define their own operation, which is not limited to sums, products, etc. We also allow the iterator variable to be of arbitrary type, not restricted to integers. The axioms we generate are general to all of these types and operations, allowing more flexible use cases. We also support automatic decompositions with no additional user inputs required.

% Force a new page: without this, the quote would appear on the
% previous page.
\newpage

\chapter{Background}
In this chapter, we explain the background knowledge required for understanding the rest of the thesis.


\section{Program Verification and Viper}
Program verification is the process of proving the correctness of a program according to its formal specifications. In this process, the programmer first annotates the source program with assertions written in a specification language. A program verifier then checks the annotated program statically, \ie, without executing the program, to verify the annotated assertions. Before the verification, certain verifiers will transform the source program into a verification language program through a verification frontend. For example, the Prusti \cite{prusti} and Gobra \cite{gobra} verifiers transform Rust and Go programs, respectively, into the Viper verification language.

Viper \cite{viper} is a verification infrastructure which provides an intermediate verification language and two built-in verifier backends. The first backend, Silicon, relies on symbolic execution, and the second backend, Carbon, generates verification conditions in Boogie, another intermediate verification language. Both ultimately use the SMT solver Z3 to check the validity of the input Viper program. Typically, we use the word \emph{Viper} to refer to the intermediate verification language and \emph{Viper backends} to refer to the two backends as described. A Viper frontend, such as Gobra or Prusti, extends a general-purpose programming language with a specification language for annotations and ultimately compiles the program from the source language into Viper for verification. 

\section{Viper Framing and Permissions}
A key feature of Viper is its support for permissions logic, which is a solution to the frame problem of heap-modifying programs.

\subsection{The Frame Problem}
The frame problem is a problem that arises in modular program verification when it becomes too verbose to describe how a method or function changes the state of a program. In modular program verification, a call to a method is summarized by a set of preconditions and postconditions rather than explicitly duplicating the body of the method. In this way, the method body can be abstracted and described modularly by its specifications.  Concretely, a verifier translates a method call into an assertion of the preconditions, \ie, the requirements of the methods, and an assumption of the post-condition, \ie, the effects of the method. 

However, in the presence of a global state like the heap, any method body could modify that state. Without any additional specification, one would need to conservatively assume that the entire global state has changed once a method call occurs. 

For example, suppose the method "modifyState" modifies an array "a" at index "i", and suppose that the array elements are stored in the heap, making them global. "a[i]" refers to array element indexed "i".
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
method modifyState(a: Array, i: Int)
    requires length(a) > i
    ensures a[i] == 10
{
    a[i] := 10
    a[0] := 0
}
\end{lstlisting}
The postcondition of the method ensures that the array element at index "i" has a value of 10, but the method body also modifies the element at index 0. The method specifications (without the body) do not specify the values at other array indices, so for soundness, the method caller has to assume that all elements could have changed.

Alternatively, the programmer would have to write more verbose pre/post-conditions to describe the unchanged portion of the global state. For example, the modifyState method specifications must be rewritten to include a framing axiom, \ie, an axiom that prescribes the states that remained unchanged.
\begin{minipage}{\linewidth}
\begin{lstlisting}
method modifyState(a: Array, i: Int)
    requires length(a) > i
    ensures a[i] == 10
    ensures forall j: Int ::   
        j != i && j >= 0 && j < length(a)  ==> 
         a[j] == old(a[j])  
            // old means the state before the method execution 
{
    a[i] := 10
    a[0] := 0
}
\end{lstlisting}
\end{minipage}
This becomes excessive very quickly because additional assumptions must be added for all heap locations, and every modular heap-modifying component must contain these axioms. In fact, we cannot write assumes describing all other heap data in a modular way. The example method only concerns array indices 0 and "i", but the postconditions must describe the values at every other index. This is not modular.

\subsection{Permissions}
To mitigate the frame problem, Viper uses a \emph{permission logic} to describe heap location permissions that a method may have. A permission is a key to access a heap location, and each heap location has exactly 1/1 permissions. Modular program components in Viper, such as methods, functions, and loops, require permission for specific locations to read and/or modify them. They can also have fractional permissions, which may allow components to read the heap locations but not modify them. To write, the method needs to hold full permissions, \ie, 1/1. Fractional permissions are particularly useful for modelling concurrent programs with methods accessing the same global state concurrently.

Framing is achieved when a method has permission to heap locations across two states. Those heap locations are considered unchanged between two states unless modified explicitly by the method itself. If the current method holds some permission to a heap location, other methods would not have full permission, so they cannot possibly modify that state. 
% Permissions frame the method implicitly because the method can only use and/or modify the global state that it has permissions to; any other global state is assumed to be unchanged by the method. 
Going back to the "modifyState" example, we can include the permissions in the method's pre/post conditions, as shown in \cref{modifyState}. 

\begin{figure}[h]
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
method modifyState(a: Array, i: Int)
    requires length(a) > i
    requires acc(a[i]) && acc(a[0]) // takes permissions
    ensures a[i] == 10
    ensures  acc(a[i]) && acc(a[0]) // gives permissions
{
    a[i] := 10
    a[0] := 0
}
\end{lstlisting}
\caption{modifyState Example with Permissions}  \label{modifyState}
\end{figure}

Suppose that another method calls "modifyState".
The method call only takes permissions to array indices 0 and "i". Any other index can be assumed unchanged, provided the caller has these permissions. The specification does not need to include an explicit framing axiom denoting that all other indices remain the same. Unrelated heap locations, relevant only to the caller, do not need to be described in the "modifyState"; this makes the method specifications modular

From a modular component's perspective, giving permissions away, such as by calling a method, means losing knowledge about a heap location. Likewise, if a method holds permissions for a heap location, it knows that the location could not have been modified while it currently holds permissions. There is only 1 permission per heap location, and any other method would have needed permissions to modify it. 

Notice also that the "modifyState" method takes permissions in the "requires" but also gives those permissions back in the "ensures." From the caller's perspective, it gives "modifyState" permissions and gets the permissions back after that method is finished.

The kind of permission logic Viper employs is more precisely called implicit dynamic frames. ``Frames'' refers to framing modular program components, like method calls, function calls, and loops. ``Implicit'' refers to how the frames are implicitly determined by the permissions given to a method, and methods are framed to the portion of the global state to which they have permissions. No explicit framing axioms are required. The frames are ``dynamic'' because the permissions may depend on values in the program, so permission checks must be done with calls to the SMT solver.

\section{Viper Basic Features and Syntax}
Next, we will discuss some of the fundamental Viper constructs needed for future reference when demonstrating our design with Viper code. The code examples that we have described so far are in pseudocode. So, in this section, we will introduce the correct Viper constructs to transform our pseudocode example to real Viper. Let us use the previous pseudocode code snippet in \cref{modifyState} as the leading example.

\subsection{Methods}
Our "modifyState" snippet above is a method declaration. A Viper method is a sequence of statements where much of Viper's verification happens. Viper handles a method call modularly by only using the method's pre-conditions and post-conditions, which are denoted in the method declaration with keywords \lstinline{requires} and \lstinline{ensures}, respectively. Viper verifies that the method satisfies its specifications once, and any calls to the method get abstracted by its specifications.

In verifying a method, Viper first assumes the preconditions, verifies the body, and then verifies that the post-condition holds. Verifying and calling a method have opposite behaviours. Verifying a method means inhaling \footnote{For now, think of exhales as asserts and inhales as assumes. We discuss their differences later.} its preconditions and exhaling its postconditions. Calling a method means exhaling the preconditions and inhaling the post-conditions.  "assert" will check the condition written in the statement, and "assume" will assume the conditions as true. If another method, "caller," calls "modifyState", the call essentially gets translated into exhales and inhales, as in the following example.
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
method caller(a: Array, i: Int)
    requires length(a) > i
    requires acc(a[i]) && acc(a[0])
    ensures  acc(a[i]) && acc(a[0])
{
    // modifyState(a,i) desugars into exhales and inhales of its specifications.
    exhale length(a) > i
    exhale acc(a[i]) && acc(a[0])
    inhale a[i] == 10
    inhale  acc(a[i]) && acc(a[0]) // gives permissions
}
\end{lstlisting}

\subsection{Statements}
Inhales and exhales are examples of Viper statements. Viper statements include common programming operations such as variable declarations, variable assignments, if-loops, and while-loops. 

Here are some Viper statements that are not common programming operations.
\begin{itemize}
    \item \lstinline{assert A}: Check the permissions and propositions in A.
    \item \lstinline{assume A}: Assume the permissions and propositions in A.
    \item \lstinline{exhale A}: Remove the permissions and assume the propositions in A. 
    \item \lstinline{inhale A}: Add the permissions and assume the propositions in A.
\end{itemize}

Asserts and assumes are statements that do not modify permissions, whereas inhales and exhales do. When permissions are not relevant, asserts and exhales can be used interchangeably, as with assumes and inhales. 

\subsection{Viper State}
Inhales and exhales could modify the current state but assumes and asserts do not.

According to the Viper tutorial, the Viper program state includes 
\begin{itemize}
    \item All variables in scope, such as the local variables, input parameters, and return variables.  
    \item Permissions to heap locations
    \item Values stored in heap locations
\end{itemize}

One can use old expressions like "old[a](...)" to denote the value at a certain state, defined by some "label a". Old expressions, however, only work for heap values, not general variables.

Statements that modify state are impure, while those that do not are pure. Variable and heap reassignment with ":=" are obviously impure. However, statements that modify permissions via "inhale" and "exhale" are also impure.

\subsection{Gaining and Losing Permissions Through Accessibility Predicate}
Differences between asserts and exhales appear when accessibility predicates are used, as denoted by \lstinline{acc(A)} where "A" mentions a heap location. For example, the "caller" method body has the statement "exhale acc(a[i]) && acc(a[0])". An accessibility predicate describes access to a specific heap location and is considered an impure expression, because inhaling or exhaling the predicate may modify the current permissions. Inhaling an accessibility predicate means gaining permissions to the heap location, and exhaling an accessibility predicate means losing permissions. 

\subsubsection{Perm expressions}
Accessibility predicates are also allowed inside assert and assume statements, but this is generally discouraged due to their impurity. In such scenarios, it is preferable to use perm expressions, \lstinline{perm(A)}, which represent permissions to a heap location as a fraction. Perm expressions are pure, so inhaling or exhaling them will not modify the current permissions available. Also, perm expressions are fractions, while accessibility predicates are boolean values by definition, so their usages are slightly different.

\subsubsection{Separating Conjunctions}
Accessibility predicates can be conjoined together using the \lstinline{&&} operator, as in "exhale acc(a[i]) && acc(a[0])". This represents a separating conjunction in Viper. For other expressions, the "&&" operator behaves like the ordinary logical \& operator. But on accessibility predicates, the separating conjunction requires that the permissions must be separate, \ie, we have the sum of the conjoined permissions. 

For example, "exhale acc(a[i]) && acc(a[i])" translates to checking that the state contains 1 permission for heap location "a[i]" and also another 1 permission for "a[i]." This would require 2 permissions to the same location, "a[i]," which is not possible by definition. So, the exhale would fail and raise a verification error.

This separating property of the "&&" operator helps prevent complications from aliasing. If x and y are aliases of the same reference, \lstinline{exhale acc(x.F) && acc(y.F)} would fail. Likewise, \lstinline{inhale acc(x.F) && acc(y.F)} implies that x and y are not aliases.  

% where \lstinline|A| contains a field access, such as \lstinline|r.val|. An accessibility predicate describes access to a specific heap location and is considered an impure expression. Inhaling an accessibility predicate means gaining permissions to the heap location, and exhaling an accessibility predicate means losing permissions. 

\subsection{Quantified Permissions}
Although multiple accessibility predicates can be conjoined with the "&&" operator, sometimes we need to use them inside a universal quantifier. For example, we may want to express having permissions from array indices 0 up to an unbounded array length. Explicitly writing out each index is impossible, so we want to write the accessibility predicates inside a quantifier, like in the following code snippet. 
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
method hasAllArray(a: Array, i: Int)
    requires forall i: Int :: i >= 0 && i< len(a) ==> acc(a[i])  
        // This precondition expresses having permissions to all array locations.
\end{lstlisting}
Accessibility predicates that appear inside a universal quantifier are called quantified permissions. The usual form of quantified permissions is "forall x: T :: c(x) => acc(r(x).F, p(x))", where "T" is an arbitrary type, "c" is some boolean function over "x", "r(x).F" is some heap location, and "p" is a function denoting the fractional permissions at each "x". "c" can be thought of as a filter limiting the quantified permissions over the set of input "x" that passes the filter.

In our "hasAllArray" example, "forall x: T" is  "forall i: Int". The boolean expression "c(x)" is " i >= 0 && i< len(a)", and the access predicate "acc(r(x).F, p(x))" is "acc(a[i])"; here, p(x) is omitted, which gets interpreted as 1 full permissions in Viper.

Now, we have been using the pseudocode "a[i]" to represent the array element at index "i", but arrays do not actually have native support in Viper. To model arrays properly, we need to use Viper domains.

% Viper requires that receiver function "r" must be provably injective over "x" that passes through the filter "c".  The verifier would perform an injectivity check for each program point where quantified permissions appear. With injectivity, each instance of "x" maps to a different reference, so each instance of the accessibility predicate denotes access to a different heap location.

% Injectivity of the receiver function will also appear in our formalization of a heap-dependent fold. 

\subsection{Domains}
An array type can be declared as a Viper domain. Viper domains allow the definitions of new types, functions, and axioms. A Viper domain modelling arrays is shown below:

\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
domain Array {
  function loc(a: Array, i: Int): Ref
  function len(a: Array): Int

  axiom len_nonneg {
    forall a: Array :: { len(a) }
      len(a) >= 0
  }
}
\end{lstlisting}

Above, we define a new type \lstinline|domain Array| and two new functions. These functions are uninterpreted functions without bodies, so we often must write axioms to describe their behaviour. In this example, the axiom, \lstinline|len_nonneg|, says that len(a) is always non-negative. Notice that the forall quantifier has a trigger "{ len(a) }"; we will discuss triggers in more detail later.

Now, consider the function "loc(a: Array, i: Int)," which returns a "Ref" given an array and an index. Previously, we used "a[i]" to refer to an array element, which we supposed was typed "Int". In real Viper, we would instead write "loc(a,i)", but this gives a "Ref" not an Int. To actually model a global variable of a predefined type, we would need to link a field to a reference.

A domain can also use type variables to allow for polymorphism. For example, the following domain declares a type variable "A" that can also be used within the domain definitions.
\begin{lstlisting}
domain Example[A] {
  function f(a:A) : A
}
\end{lstlisting}
The function "f" is polymorphic; it takes an input of any type "A" and outputs the same type. Viper also allows declaring functions outside of domains. Those functions can depend on the heap and can have bodies, but notably, they cannot use type variables like domain functions. We discuss more about these non-domain functions later.

\subsection{Fields}
In Viper, global variables are represented by fields inside objects. Objects are represented with pointers of type \lstinline{Ref}, and all objects contain all fields. Together, objects and fields point to heap locations, also called field locations. Intuitively, one can conceptualize the heap as a mutable map of type \lstinline{(Ref, Field A) $\rightarrow$ A}. In fact, the Viper backends would model the heap as exactly this map!

Fields can be declared at the top level using the keyword \lstinline|field|. For example, \lstinline|field val: Int| declares a field of type \lstinline{Int} called \lstinline{val}. The field \lstinline|val| of some reference \lstinline|r| is represented by \lstinline|r.val|, which can then be read and/or written to, given we have permissions to that heap location.

So, the pseudocode representing an array element "a[i]" should instead be written as "loc(a,i).val" in Viper.

\section{SMT Solving and Triggers}
To check the "exhale" and "assert" statements inside methods, Viper relies on generating verification conditions for an SMT solver.

\subsection{Satisfiability Modulo Theories (SMT)}
An SMT solver is a tool that solves satisfiability modulo theories problems or SMT for short. SMT problems are a generalization of the Boolean satisfiability problem, SAT for short. A SAT problem involves finding a solution to a Boolean formula by assigning a value to each variable. A model is a map from all variables to their respective truth value. If a solution exists, the formula is considered satisfiable. For example, $A \land B$ is satisfiable with the model mapping $A = \textit{true}$ and $B = \textit{true}$. A formula is considered valid if it always holds, regardless of the model. For example, $A \land \neg A$ is valid because the formula evaluates to true regardless of $A$'s truth value. A formula is considered unsatisfiable if no satisfying model exists. 

SMT extends the SAT problem by adding theories, allowing for more complex expressions such as addition, equality, and inequality. Numbers and functions are also allowed in addition to Boolean variables. For example, $f(x) + x > 1$ is an SMT formula, and solving it would require finding the assignments to number $x$ and function $f$ such that the formula holds. Satisfiability, unsatisfiability, and validity also mean the same in SMT solving.

\subsection{SMT Solving in Viper}
In Viper, we want to check each "assert" statement given some set of assumptions, either from "assume" or explicit axioms. To do so, Viper generates the required verification conditions and gives them to Z3, an SMT solver from Microsoft Research. Generally, we want to check whether assumptions "A" ``entails'' assertion "B", \ie, $A \models B$. Entailment means that for all models $M$ such that $M$ satisfies $A$, then $M$ also satisfies $B$. Consider the following Viper program.
\begin{lstlisting}
method smt_example(x: Int) {
    assume x > 10
    assert x != 0
}
\end{lstlisting}
We want to solve the entailment $x > 10 \models x \neq 0$. It is known that checking entailment $A \models B$ is equivalent to checking the validity of implication $A \rightarrow B$, which is also equal to checking the unsatisfiability of  $A \land \neg B$. So, for this "smt_example" method, the Viper verifier would give the conditions $x > 10\land \neg( x \neq 0)$ to the Z3 solver to check for unsatisfiability. 

\subsection{Triggers in Quantifiers}
Viper also allows writing universal quantifiers and existential quantifiers. Universal quantifiers (also called for all quantifiers) are written as $\forall x: \text{Int}. A$ denoting that $A$ holds for all possible integer $x$. Existential quantifiers are written as $\exists x: \text{Int}. A$, denoting that an integer $x$ exists with property $A$.

Universal quantifiers are particularly useful in Viper for writing domain axioms, as in the array encoding domain example below.
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
domain Array {
  function loc(a: Array, i: Int): Ref
  function len(a: Array): Int

  axiom len_nonneg {
    forall a: Array :: { len(a) }
      len(a) >= 0
  }
}
\end{lstlisting}

Technically, a for-all quantifier represents a possibly infinite conjunction over the quantified type. So, the above axiom is saying 
\begin{lstlisting}
len(a_1) >= 0 && len(a_2) >= 0 && len(a_3) >= 0 && ...
\end{lstlisting}
including all possible arrays "a_i : Array". The SMT solver cannot instantiate every possible "a_i" from the quantifier, so it instead relies on the trigger, which is enclosed inside the curly braces "{...}". The term "len(a_i) >= 0" will only instantiate as an assumption if elsewhere "len(a_i)" is mentioned. The triggering term could be written in a statement by the user, or could be instantiated by a different axiom. 

The SMT solver will also match the trigger using know equivalence. For example, suppose an axiom uses "{g(f(x))}" as a trigger. Then somewhere else, suppose there is an assumption that "f(x) = 5". Then,  the term "g(5)" will still match the trigger, because the SMT solver can replace "5" with "f(x)" knowing they are equal. This is called \emph{e-matching}, where equivalence is also used for trigger matching. Moreover, in the SMT solver's backend, there is a data structure call an \emph{e-graph} that helps track equivalence between terms. Terms that exist as an assumption during are considered added to the e-graph. 

\subsection{Choosing Triggers}
Triggers must be chosen carefully to ensure that the axioms are instantiated when necessary. For example, 
\begin{lstlisting}
method not_triggered() {
    assume forall x: Int :: {f(x)}
        f(x)
    assume forall x: Int :: {f(x)}
        f(x) ==> false
    assert false 
}
\end{lstlisting}
In the example method "not_triggered", there are two axioms with universal quantifiers; one says that "f(x)" is always true and the other says that "f(x)" implies false, for any integer "x". Together, this should prove false. However, the assertion of false fails because neither axiom ever instantiates; the term "f(x)" does not appear anywhere else. Explicitly writing out any instance "f(x)" would make the proof go through, like in the following example.
\begin{lstlisting}
method triggered() {
    assume forall x: Int :: {f(x)}
        f(x) ==> false
    assume forall x: Int :: {f(x)}
        f(x)
    var y: Int     // added
    assert f(y)    // added
    assert false 
}
\end{lstlisting}
Though we want the quantifier to instantiate whenever necessary, we also want to avoid matching loops, which occur when a term matches a trigger and causes the axiom to instantiate indefinitely. We will discuss controlling matching loops in section ??? where we design our axioms in the plugin.

\section{Other Relevant Viper Features}

% \begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
% field val: Int
% method modifyVal(r: Ref, i: Int)
%     requires acc(r.val)
%     ensures acc(r.val)
% {   
%     r.val := i
% }
% \end{lstlisting}

% In this example, a field \lstinline{val} gets defined, and the field location \lstinline{r.val} gets modified inside the method. Notice the method's precondition inside the requires clause features an accessibility predicate \lstinline{acc(r.val)}, denoting full permissions to that field location. We discuss more details on permissions in the next section. 

\subsection{Predicates and Functions}

A predicate in Viper is a top-level declaration that represents parameterized assertions. Its definition could contain recursive calls (calls to itself) and resource assertions, such as access to a heap location,\ie, field. A predicate instance could represent access to a heap data structure. For example, we could model a linked list with a predicate as follows:

\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
field elem: Int
field next: Ref

predicate list(this: Ref) {
  acc(this.elem) && acc(this.next) &&
  (this.next != null ==> list(this.next))
}
\end{lstlisting}

The \lstinline|elem| field models the integer stored at each list node, and the \lstinline|next| field models the pointer to the next list node. The list instance with input \lstinline|this| means that we have access to fields \lstinline|elem| and next of \lstinline|this|, and if the \lstinline|next| reference points to a node, we also have the list instance with that node as input. In short, if the predicate \lstinline|list(this)| holds, then we have access to the whole list, starting with node \lstinline|this|. 

Next, we may also be interested in certain properties of the list, such as the sum of all the elements of the list. To express that sum, we use the following function definition:

\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
function listSum(l: Ref) : Int
    requires list(l)
{
    unfolding list(l) in l.next == null ? l.elem : 
        l.elem + listSum(l.next)
}
\end{lstlisting}

A function defined outside of a domain can have its own body and pre and post-conditions. However, its body must be a pure expression, unlike a method body which may contain a list of statements. A non-domain function in Viper is also called a heap-dependent function because it can access heap elements via fields and permissions.

In the above code snippet, the \lstinline|unfolding| keyword allows the program to access terms inside the predicate instance. In the \lstinline|listSum| function body, we read \lstinline|l|’s next field, whose permissions we acquire from unfolding the predicate \lstinline|list(l)|. It is worth noting that the function is defined recursively to match the recursive structure of the list defined in the \lstinline|list| predicate. Together, predicates and functions allow us to model recursive data structures and describe their properties. However, for random-access data structures, it is more appropriate to use quantified permissions.

\chapter{Design}
Now that we have explained most of the background information, we proceed to discuss heap-dependent folds, which is the main feature we intend to implement in Viper. 
% Some of the background details were omitted for brevity, but we will explain them more thoroughly when necessary.
In this chapter, we will formally define heap-dependent folds and discuss the axiomatizations to allow reasoning about them in Viper. Although the design is for Viper, in this chapter we avoid going too deeply into the implementation details, keeping things more mathematical. The next chapter will discuss the Viper axioms we designed in more detail, including selecting triggers, controlling instantiations, \etc.

\section{Heap-Dependent Fold}
We define fold as a function that takes a set of values and returns a value describing a property about the input set. As such, a fold might also be called an aggregate property or a comprehension. For our purposes, the input set of values are values extracted from the heap, so the more precise name is heap-dependent fold.

\subsection{Fold vs Comprehension}
This should not be confused with set comprehensions, commonly found in maths, or list comprehensions, commonly found in general-purpose programming languages. A list comprehension is a way to construct a new list given an existing list; the result is a list, not a value. Similarly, a set comprehension constructs a set from an existing set. Conversely, a fold will combine values inside a list or set with some pre-defined operation. A fold is more general than a comprehension because the pre-defined operation could also construct a set. Nevertheless, previous literature that discusses the same feature we implement calls it ``comprehension'', so perhaps in the context of program verification, comprehension also describes a fold. 

\subsection{Heap-dependent Fold Definition}
Consider the following notation for heap-dependent folds.
Let $$\textrm{fold}_{\sigma}[o]( m(r.F) \mid f)$$
be the fold over all heap elements defined by filter $f$, receiver $r$, and field $F$, where heap $\sigma$ is the current heap, and operator $o$ is the operator object containing a binary operation $(B, B) \rightarrow B$ and an identity element $i_0 : A$ for that operation. 
Filter $f$ can be considered a set of unordered elements $A$. The receiver $r$ is a function $A \rightarrow \textrm{Ref}$. The mapping is a function from the field type $V$ to output $B$, \ie, $V\rightarrow B$. Altogether, this expresses an unordered fold, where the binary operation must be commutative and associative. 

Let us discuss heap $\sigma$ in more detail. In Viper, there are two components to the heap in the program state:
\begin{itemize}
    \item The heap value map: a total map representing values stored on the heap, typed \lstinline{(Ref, Field A) $\rightarrow$ A}
    \item The permission mask: a total map representing permissions to each heap location, typed \lstinline{(Ref, Field A) $\rightarrow$ Perm}
\end{itemize}
These two can be combined into one partial map $\sigma$, typed \lstinline{(Ref, Field A) $\rightarrow$ A}. When permissions are lost to a heap location, we delete the appropriate keys in $\sigma$. When permissions are gained, we add the new keys to $\sigma$, with unspecified default values. 

At different program points, a heap-dependent fold typically varies only in the heap $\sigma$ and filter $f$. For example, suppose the programmer is interested in the sum of an integer array in the method. The array elements could be modified, changing $\sigma$, or perhaps we can take a subarray, changing filter $f$. So, in contexts where other arguments obviously remain constant or are not relevant to the discussion, we omit $r$, $F$, $m$, $o$, leaving only
$$\textrm{fold}_{\sigma}(f).$$

\subsubsection{Equality of Heap Snapshots}
Two folds with two different heap arguments could still be equal if the relevant heap portions are equal, \ie, their snapshots. 

Consider the case when two heap-dependent folds have the same arguments other than the heap \ie, one has heap $\sigma_0$, the other $\sigma_1$. If $$\forall e \in f. \sigma_0[r(e), F] = \sigma_1[r(e), F],$$ then
$\textrm{fold}_{\sigma_0}(f) = \textrm{fold}_{\sigma_1}(f)$. That is, if two heap-dependent folds differ only in their heap argument and their respective heap portions defined by the filter, receiver, and field are equal, then the two folds are equal. 

We define a heap snapshot as a portion of the heap limited by the filter, receiver, and field. Formally,
$$\textrm{snap}^{r.F}_{f}(\sigma) = \{ e \rightarrow \textrm{value} \mid ([r(e), F] \rightarrow \textrm{value}) \in \sigma \land e \in f\}.$$ For convenience, we have changed the key from a "[Ref, Field]" to an element of the filter $e \in f$, but it expresses the same thing considering the receiver $r$ is injective with respect to the filter $f$. Also, we removed the field from the key because we specified the field $F$ as the only relevant field. These little details are not completely relevant now, but this snapshot definition will be crucial in the implementation detail of the next chapter.

We write
$$\textrm{snap}^{r.F}_{f}(\sigma_0) = \textrm{snap}^{r.F}_{f}(\sigma_1)$$ to denote two heaps equal with respect to receiver $r$, field $F$, and filter $f$, \ie, their snapshots are equal. So, $$\forall \sigma_i, \sigma_j \ldotp \textrm{snap}^{r.F}_{f}(\sigma_i) = \textrm{snap}^{r.F}_{f}(\sigma_j) \implies \textrm{fold}_{\sigma_i}(f) = \textrm{fold}_{\sigma_j}(f).$$ 

Technically, one could modify the fold notation to include the snapshot, combining receiver $r$, field $F$, filter $f$, and heap $\sigma$ into one expression: $$\textrm{fold}[o]\{m(\textrm{snap}^{r.F}_{f}(\sigma)\},$$ but we will use our previous notation for consistency.

\subsubsection{Evaluation of Heap-dependent Fold}
Intuitively, a heap-dependent fold evaluation simply aggregates all values on the heap defined by the filter, receiver expression, and field. The filter produces a set of relevant indices, and the receiver function is applied to those indices, giving a set of references. These references, paired with a field, point to some heap locations, and reading them would give a set of values stored on the heap. Two such values can then be chosen arbitrarily to be combined using the binary operator. The values are combined repeatedly until only one value remains. If there is no value at the very start, the output is the identity $i_0$ of the operator. 

The only addition to this description is the presence of a mapping function. Often, it may be useful to apply a function to each heap value before performing the combine prescribed by the binary operator. For example, suppose we want to collect all the integers stored in a heap chunk into a set. Then, the binary operator should output an integer set, which would require each operator input to be an integer set, as enforced by the operator's type signature $(A, A) \rightarrow A$. However, each heap location stores an integer, which causes a mismatch of types. We can circumvent this by introducing a mapping function that constructs a singleton integer set from an integer.

Altogether, we can abstract the heap-dependent fold operation into 5 steps:
\begin{enumerate}
    \item Filter: construct a set of indices that are relevant, as defined by some filtering condition
    \item Receiver: map the set of indices to a set of references. Each index maps to a different reference; the receiver is injective.
    \item Field: combine the references with the specified field to get heap locations
    \item Mapping: read the values stored in the heap locations and apply a mapping function to them.
    \item Operator: combine the mapped values together using the defined binary operator iteratively until only one value remains.
\end{enumerate}

The evaluation of a heap-dependent fold is defined recursively as follows:
\begin{itemize}
    \item If the filter $f$ is empty, the fold evaluates to $i_0$, where $i_0$ is the identity element.
    \item If the filter $f$ is a singleton set $\{x\}$, the fold evaluates to $m(r(x).F)$, where $r$ is the receiver function, $F$ is the field, and $m$ is the mapping function.
    \item If the filter $f$ can be deconstructed such that $f = f_1 \uplus f_2$, where $f_1$, $f_2$ are filters and $\uplus$ represents the disjoint union, then  $$\textrm{fold}_{\sigma}[o]( m(r.F) \mid f)  = \textrm{fold}_{\sigma}[o]( m(r.F) \mid f_1)  \oplus_o \textrm{fold}_{\sigma}[o]( m(r.F) \mid f_2),$$ where $\oplus$ is the infix notation for the binary operator defined by $o$. This rule splits the fold into two and combines them using the binary operator $o$.
\end{itemize}

The third rule above describes a decomposition, which splits a filter into two smaller filters, $f_1$ and $f_2$. The challenge in reasoning about the fold values is finding the two smaller filters suitable for the decomposition. In practice, we only find one smaller filter $f_1$. If we have an existing filter $f$ and find a subset $f_1$ of $f$, we can derive an $f_2$ as a set-minus, \ie, $f_2 = f - f_1$. 

However, there is an unbounded number of indices for an unbounded data structure. Exploring every possible decomposition given an unbounded set of indices is impractical. This is one key challenge to our design in Viper.

\subsection{Well-definedness Requirements}
There are a couple of requirements to make the heap-dependent fold sound. Firstly, the types have to match. The filter $f$ must have type $\textrm{Set}[A]$, and the receiver has type $A \rightarrow \textrm{Ref}$, and given that the field has type $V$, the mapping has type $V \rightarrow B$. The binary operator then has type $(B, B) \rightarrow B$ and has an identity typed $B$. Implementation-wise, this all gets encoded as part of the Viper language typechecker. 

There are other requirements that need to be checked separately by the SMT solver:
\subsubsection{Operator must be associative, commutative, and have an identity}
In an unordered fold, each element in the input set can be combined in any arbitrary order, mimicking the random access nature of a data structure. So, the operator must be provably associative and commutative. 

In the case where the input set is empty, the fold must still be evaluated to a value for soundness. Following the third rule of a fold evaluation, \ie the decomposition rule, we can always decompose a filter into itself and the empty set. So,
$$\textrm{fold}_{\sigma}(f) = \textrm{fold}_{\sigma}(f) \oplus \textrm{fold}_{\sigma}(\{\})$$ would hold for all $f$ if the fold on the empty set evaluates to the identity of the operator. 

\subsubsection{Receiver must be injective on the filter}
Each element of the filter set must map to a different reference. This gets checked as a function precondition in the Viper encoding. This greatly simplifies the implementation because each heap location maps to one element in the filter. If one heap location gets reassigned, the decomposition would involve extracting that one element from the original filter.

We designed our feature to work mostly with quantified permissions, which already require the injectivity of the receiver in Viper. So, this requirement is not really an additional restriction, provided the user intends to use quantified permissions.

\subsubsection{Current state must have enough permissions to all heap locations defined by the filter and receiver}
A Viper method cannot read values from heap locations it does not have permission to. So, a fold can only be evaluated at a program point if it has permissions to all the heap locations defined by filter $f$ and receiver $r$.

Technically, in Viper, the heap and permissions are separate maps. However, we have overloaded our heap notation $\sigma$ as a partial heap representing both the heap and the permissions. If permissions have changed, then we consider the heap $\sigma$ to have changed too. One can think of $\sigma$ as representing the heap portion with permissions.

\subsection{Connection to Quantified Permissions}
Recall that quantified permissions in Viper have the canonical form "forall x: T :: c(x) => acc(r(x).F, p(x))". Some design choices were adopted from this to heap-dependent folds.

\begin{enumerate}
    \item The filter set $f$ is analogous to "forall x: T :: c(x)", which defines a set of elements typed "T".
    \item The receiver function $r$ is an abstraction of the receiver expression "r(x)" in quantified permissions.
    \item Field $F$ denote the relevant field.
    \item Receivers $r$ must be injective in both quantified permissions and heap-dependent folds, with respect to filter $f$ representing boolean expression "c(x)." 
\end{enumerate}

\section{Singleton Decomposition}
Suppose we have a heap-dependent fold $$\textrm{fold}_{\sigma_0}[o]( m(r.F) \mid f)$$ at a program point with heap $\sigma_0$. Given that we do not modify any other argument, we abbreviate this as $$\textrm{fold}_{\sigma_0}(f).$$ Suppose the heap has been modified, and at a new program point, we have heap $\sigma_1$. What are some facts that we can deduce about $\textrm{fold}_{\sigma_0}(f)$ and $\textrm{fold}_{\sigma_1}(f)$?

To answer this, we consider scenarios where the heap can be changed in Viper. There are only three such scenarios.
\begin{enumerate}
    \item A heap location gets explicitly reassigned with the ":=" operator, \eg, "r.val := 0"
    \item The program state exhales permissions to a portion of the heap. 
    \item The program state inhales permissions to a portion of the heap. 
\end{enumerate}
A method call is considered a combination of scenarios 2 and 3 because the Viper verifier abstracts a method call as exhales and inhales. The first scenario is quite straightforward, while the two latter ones are more tricky. Losing and gaining permissions technically does not modify the heap, but instead the permission mask of the heap. However, in our formalization, we use $\sigma$ to denote the partial map of the heap portion with permissions, which will have changed after inhales and exhales. 

Consider an example of the first scenario. Suppose we are interested in a fold with the sum operator $o_{sum}$, which is a binary operator with identity 0. Let the mapping $m$ be the identity function, so $m(A) = A$. Then, suppose we have an arbitrary $i \in f$ and encounter a reassignment "r(i).F := r(i).F + 1 ". Suppose also that the current method has permissions to all heap locations defined by filter $f$.  Let the heap prior to the reassignment be $\sigma_0$, and the heap after is $\sigma_1$. Intuitively, we can see that the sum over filter $f$ and receiver $r$ must now be 1 larger than previously. 

Now, suppose the user writes an assertion to check:
$$ \textrm{fold}_{\sigma_1}[o_{sum}](f) = \textrm{fold}_{\sigma_0}[o_{sum}](f) + 1.$$

To prove this, we can construct a new filter by removing element $i$ from the original filter $f$. This gives us the decomposition of $f$:
$$f = \left( f - \{i\}\right) \uplus \{i\}$$

With this decomposition, we can use the third rule of evaluating a heap-dependent fold, which gives, for $\sigma_0$
$$\textrm{fold}_{\sigma_0}(f) = \textrm{fold}_{\sigma_0}(f - \{i\}) \oplus_{sum} \textrm{fold}_{\sigma_0}(\{i\})$$ and similarly for $\sigma_1$,
$$\textrm{fold}_{\sigma_1}(f) = \textrm{fold}_{\sigma_1}(f - \{i\}) \oplus_{sum} \textrm{fold}_{\sigma_1}(\{i\}).$$  

Notice, however, that $\textrm{fold}_{\sigma_0}(f - \{i\})$ and $\textrm{fold}_{\sigma_1}(f - \{i\})$ are equal, because only heap location "r(i).F" has been modified. Filter set $f - \{i\}$ does not include element $i$, so the heaps $\sigma_0$ and $\sigma_1$ are identical with respect to this filter. Formally, we know that 
$$\forall e \in \left(f - \{i\}\right). \sigma_0[r(e), F] =  \sigma_1[r(e), F],$$ \ie, the heap portions defined by the filter are equal. Or in order words, $$\textrm{snap}^{r.F}_{f - \{i\}}(\sigma_0) = \textrm{snap}^{r.F}_{f - \{i\}}(\sigma_1).$$

We need injectivity to derive this equality. $r(i)$ maps to a unique reference, and injectivity guarantees that there is no other $j \in f \land j \neq i$ such that $r(i) = r(j)$. So, removing $i$ from $f$ means nothing else points to the same reference.

Furthermore, Viper permissions also guarantee that no other state change has occurred during the reassignment and that $\sigma_0$ and $\sigma_1$ differ only at $r(i).F$. Remember that we assumed the current method holds permissions to all heap locations defined by the filter, which means nothing else could modify these locations other than the method. 

Next, consider the second fold with the second filter in our decomposition, $\textrm{fold}_{\sigma_0}(\{i\})$. The filter is a singleton, so by definition of a heap-dependent fold, this evaluates to $\sigma_0[r(i),F]$ (the mapping is the identity function). Similarly, we consider $\textrm{fold}_{\sigma_1}(\{i\})$, which evaluates to $\sigma_1[r(i),F]$. Now, recall the two heaps occur before and after the reassignment "r(i).F := r(i).F + 1 ". By definition of the reassignment, we know 
$$\sigma_1[r(i),F] = \sigma_0[r(i),F] + 1.$$

So, $\textrm{fold}_{\sigma_1}(\{i\}) = \textrm{fold}_{\sigma_0}(\{i\}) + 1,$. Substituting these equalities into the decomposition of $\textrm{fold}_{\sigma_1}(f)$, we get
\begin{align*} 
\textrm{fold}_{\sigma_1}(f) &= \textrm{fold}_{\sigma_1}(f - \{i\}) \oplus_{sum} \textrm{fold}_{\sigma_1}(\{i\}) \\
\textrm{fold}_{\sigma_1}(f) &= \textrm{fold}_{\sigma_0}(f - \{i\}) \oplus_{sum} (\textrm{fold}_{\sigma_0}(\{i\}) + 1) \\
\textrm{fold}_{\sigma_1}(f) &= \textrm{fold}_{\sigma_0}(f) + 1 
\end{align*}
We use the associativity of addition in that last step. 

So, we have proven what we wanted.
Let us recall the steps of our proof to relate the fold prior to the heap change to the fold after:
\begin{enumerate}
    \item Start with some $\textrm{fold}_{\sigma_0}(f)$
    \item The heap $\sigma_0$ encounters a modification to some location "r(i).F", which produces a new heap $\sigma_1$
    \item Perform a decomposition on the filter $f$ to extract two filters. The first filter denotes the portion of the heap unchanged. The second filter denotes the portion changed. In our example, we split $f$ into $\left( f - \{i\}\right) \uplus \{i\} $.
    \item Use the third evaluation rule of fold, which gives $\textrm{fold}_{\sigma_1}(f) = \textrm{fold}_{\sigma_1}(f - \{i\}) \oplus_{sum} \textrm{fold}_{\sigma_1}(\{i\}) $, and the same for $\sigma_0$.
    \item Prove that $\textrm{fold}_{\sigma_1}(f - \{i\}) = \textrm{fold}_{\sigma_0}(f - \{i\})$ using injectivity and Viper permissions.
    \item Prove that $\textrm{fold}_{\sigma_1}(\{i\}) = \textrm{fold}_{\sigma_0}(\{i\}) + 1$ using the second evaluation rule of fold, \ie, the singleton rule.
    \item Substitute equalities to deduce $$\textrm{fold}_{\sigma_1}(f) = \textrm{fold}_{\sigma_0}(f) + 1$$
\end{enumerate}
Notice that we did not need to use induction. 

\subsection{Combinatorial Explosion from Chained Singleton Decomposition}
Although the simple strategy described previously can handle a singleton heap reassignment, it can cause a combinatorial explosion when many reassignments are chained together. Consider a situation where repeated reassignments are made. Suppose we have a filter $f$ and two arbitrary integers $i, j \in f$. Then, consider a method which performs three consecutive reassignments to the heap locations of each index.
\begin{lstlisting}
{
    ...
    r(i).F := r(i).F + 1 
    r(j).F := r(j).F + 1 
}
\end{lstlisting}

Again, we want to prove that $$\textrm{fold}_{\sigma_2}(f) = \textrm{fold}_{\sigma_0}(f) + 2 $$ where $\sigma_0$ is the heap at the start, and $\sigma_2$ is the heap after the reassignments. Assume that the operator is addition and the mapping is the identity function.

Employing the same strategy, we could split the filter $f$ by removing each of $i$ and $j$.
\begin{align*} 
f &= \left( f - \{i\}\right) \uplus \{i\} \\
f &= \left( f - \{j\}\right) \uplus \{j\} \\
\end{align*}

However, this is insufficient, because neither $$\textrm{snap}^{r.F}_{f - \{i\}}(\sigma_0) = \textrm{snap}^{r.F}_{f - \{i\}}(\sigma_2)$$ nor $$\textrm{snap}^{r.F}_{f - \{j\}}(\sigma_0) = \textrm{snap}^{r.F}_{f - \{j\}}(\sigma_2).$$
The new filters do not create heap portions that are equal. We must extract both $i$ and $j$ from the filter. So, we need to allow repeated decomposition of filters that were themselves generated from a decomposition.

Repeated decomposition is very costly because the number of filters tends to grow exponentially or even worse. If we take the generated filters $(f - \{i\}), \{i\},  ( f - \{j\},  \{j\}$ and decompose them again, we potentially get eight new filters.
\begin{align*} 
\left( f - \{i\}\right) &= 
        \left( f - \{i\} - \{i\}\right) \uplus \{\} \\
\left( f - \{i\}\right) &= 
        \left( f - \{i\} - \{j\}\right) \uplus \{j\} \\
\left( f - \{j\}\right) &= 
        \left( f - \{j\} - \{j\}\right) \uplus \{\} \\
\left( f - \{j\}\right) &= 
        \left( f - \{j\} - \{i\}\right) \uplus \{i\} \\
\left(\{i\}\right) &= 
        \left(\{i\} - \{i\}\right) \uplus \{i\} \\
\left(\{i\}\right) &= 
        \left(\{i\} - \{j\}\right) \uplus \{j\} \\
\left(\{j\}\right) &= 
        \left(\{j\} - \{j\}\right) \uplus \{j\} \\
\left(\{j\}\right) &= 
        \left(\{j\} - \{i\}\right) \uplus \{i\} \\
\end{align*}

Some of the above equations may be ill-defined. Recall that to decompose filter $f$ correctly, we need $f_1$ such that $f_1 \subseteq f$. Then we can write $f = (f - f_1) \uplus f_1.$ Sometimes, it is unclear whether $f_1$ is a subset, so in practice, the SMT solver will case-split and try both options. One case might assume $f_1 \not\subseteq f$. In such cases, the ill-defined decomposition equations are not generated. 

For example, $i$ and $j$ may or may not be the same integer. So whether the subset property $\{j\} \subseteq f - \{i\}$ holds is unclear. Nevertheless, these decompositions may happen in one case, so we write them explicitly above.

Some of the equations are well-defined but do not introduce any new information. For example, we can see $\left( f - \{i\} - \{i\}\right)$ should be equal to $\left( f - \{i\}\right)$, because $i$ has already been subtracted from $f$. However, the set axiomizations in Viper still perform a case split here. The case assuming $\{i\} \subseteq f - \{i\} - \{i\}$ will lead to a contradiction eventually, but this decomposition still spawns. 

We can repeatedly decompose further until we have 16 new filters in addition to $f$:
\begin{align*} 
&f \\
&f - \{i\}  \\
&f - \{j\}  \\
&f - \{i\} - \{i\}   \\
&f - \{i\} - \{j\}   \\
&f - \{j\} - \{i\}   \\
&f - \{j\} - \{j\}   \\
&f - \{i\} - \{j\} - \{i\}   \\
&f - \{i\} - \{j\} - \{j\}   \\
&f - \{j\} - \{i\} - \{i\}  \\
&f - \{j\} - \{i\} - \{j\}  \\
&\{i\}  \\
&\{j\}  \\
&\{i\} -\{i\}  \\
&\{i\} -\{j\}  \\
&\{j\} -\{i\}  \\
&\{j\} -\{j\}  
\end{align*}
Recall that for each of these filters, there are still two folds, each with heap $\sigma_0$ and $\sigma_2$. So, there are at least 32 new folds created from the decomposition. 

This is a very large number, and we already removed some of the filters. For example, $f - \{i\} - \{i\} - \{i\}$ will not be a generated filter because we assume the SMT solver could prove $f - \{i\} - \{i\} = f - \{i\}$ immediately, discarding potential decompositions from that filter.

In practice, the SMT solver case-splitting makes this filter decomposition performance even worse. As we have explained briefly, the SMT solver has to check if $f_1 \subseteq f$ before producing $f = f_1 \uplus (f - f_1)$. This creates a case split in the proof, and each case may have its own filter decompositions. For simplicity, we can assume the number of filters doubles at each branch. Nested case splits can also happen, so the total number of folds could increase by a factor of $2^n$ for $n$ case splits. 

The number of case splits will also increase by the number of reassignments. At least, there must be one case split on the equality between each index of the reassignment. For our example, we have two indices, $i$ and $j$. The SMT solver needs to check $i = j$, giving one case split. If we have three indices $i,j,k$, the solver checks $i=j$, $i=k$, and $k=j$, giving three case splits. So, the number of case splits is equal to $m$ choose 2, where $m$ is the number of indices in the reassignments.

Though we have not formally proven the total number of filters, it obviously is very large. This example illustrates the problem with allowing repeated decomposition. We have tried running examples with three reassignments with operator ":=" using this naive strategy; Viper fails its verification, timing out at 100 seconds. Given three reassignments already timeout the verifier, it's clear that there are simply too many decompositions.

\subsection{Possible Optimization}
We tried adding optimizations to Viper's built-in set axioms. For example, we can observe that for any set $f$, $f - \{i\} - \{i\} = f - \{i\}$, or that $f - \{j\} - \{i\} = f - \{i\} - \{j\}$ (right commutativity of set minus). After we add these as axioms, Viper can verify folds inside methods with 3 reassignments but still timeouts again at 4.

\section{Local and Intermediate Decomposition}
A solution to the combinatorial explosion is using local and intermediate decompositions, allowing modular handling of each heap modification. There were two main causes of the explosion.
\begin{enumerate}
    \item Chained decomposition: a filter generated from a decomposition gets decomposed further.
    \item SMT solver case splitting: the SMT solver case splits on the pairwise equality of each index getting reassigned. Each case split doubles the number of filters.
\end{enumerate}

Once again, we consider the example with two reassignments, where the fold operator is addition, and the mapping is the identity function. 
\begin{lstlisting}
{
    ...
    r(i).F := r(i).F + 1 
    r(j).F := r(j).F + 1 
}
\end{lstlisting}
We want to prove that $$\textrm{fold}_{\sigma_2}(f) = \textrm{fold}_{\sigma_0}(f) + 2.$$

The key observation is that this is simply the example with one reassignment appended together. So, let us declare an intermediate heap $\sigma_1$, representing the heap state after the first assignment "r(i).F := r(i).F + 1". Then, the problem can be split up into proving 
$$\textrm{fold}_{\sigma_1}(f) = \textrm{fold}_{\sigma_0}(f) + 1 $$ and $$\textrm{fold}_{\sigma_2}(f) = \textrm{fold}_{\sigma_1}(f) + 1. $$ 

For the first equation, we need to perform only one decomposition of the filter $f$, \ie, $f = (f - \{i\}) \uplus \{i\}$ which gives one decomposition for each of $\sigma_1$ and $\sigma_0$. For the second equation, we again perform only one decomposition of the filter $f$, \ie, $f = (f - \{j\}) \uplus \{j\}$, which gives one decomposition for each of $\sigma_2$ and $\sigma_1$. Note that the decomposition removing $\{j\}$ does not happen with respect to $\textrm{fold}_{\sigma_0}(f)$, and the decomposition removing $\{i\}$ does not happen with respect to $\textrm{fold}_{\sigma_2}(f)$

We call this strategy the local and intermediate decomposition. First, after each heap reassignment to some index $i$, we introduce an intermediate heap $\sigma_j$ and a heap-dependent fold with argument $\sigma_j$. So, at each atomic heap modification, we have $\sigma_i$ and $\sigma_j$ representing the heap before and after the modification. Then, we locally decompose the two folds by subtracting $i$ from filter $f$, giving $$\textrm{fold}_{\sigma_i}(f) = \textrm{fold}'_{\sigma_i}(f - \{i\}) \oplus \textrm{fold}'_{\sigma_i}(\{i\})$$ and the same for $\sigma_j$.
Finally, for each reassignment, we can deduce an intermediate equality 
$$ \textrm{fold}_{\sigma_j}(f) = \textrm{fold}_{\sigma_i}(f) ???,$$ and these intermediate equality could chain together to prove properties comparing folds after arbitrary many reassignments. 

The folds generated from a decomposition should not be further decomposed for performance reasons explained previously. As such, we distinguish between a primary and a secondary fold. In our notation, a $\textrm{fold}'$, read fold prime, denotes a secondary fold. A primary fold is a fold that can be decomposed; these folds are generally folds written explicitly by the user or are generated as an intermediate fold by our strategy. A secondary fold is a fold generated by a decomposition, so the user will not directly mention it. Secondary folds do not get decomposed, while primary folds do. Both folds have the evaluation. This greatly lowers the performance cost of our implementation. In our example, $\textrm{fold}_{\sigma_i}(f)$ is primary, and the generated fold $\textrm{fold}'_{\sigma_i}(f - \{i\})$ is secondary. From hereon, when talking about decomposing a fold, we implicitly mean these primary folds.

There is a reason why we describe the strategy as intermediate and local decompositions.
\begin{enumerate}
    \item Intermediate: we introduce intermediate heaps in addition to the heaps we initially wrote.
    \item Local: the decomposition applies only to the folds with heap arguments representing the state prior and after the assignment, \ie, only two states. 
\end{enumerate}

\subsection{Modular Strategy: Nodes and Edges}
Intermediate and local decompositions allow modular handling at each heap modification. Only the heaps directly before and after the modification, \ie, $\sigma_i$ and $\sigma_j$, are relevant at each step.

With this modular strategy, the number of decompositions grows linearly with the number of heap assignments, which is a huge improvement. We no longer need to repeatedly decompose a filter generated from a decomposition, and there are no case splits on the equality of indices. Indices are instead handled separately and local to each reassignment.

However, there are some requirements to make this work. Think of each primary fold as trickling down through each node in the method, where each node is a heap-modifying statement. So, to relate the (primary) fold at the start of the method $\textrm{fold}_{\sigma_a}$ to some fold at the end $\textrm{fold}_{\sigma_z}$, there must be a path linking $\textrm{fold}_{\sigma_a}$ to $\textrm{fold}_{\sigma_z}$. Each node is responsible for generating its own local path.

For example, we can relate $\textrm{fold}_{\sigma_0}$ to $\textrm{fold}_{\sigma_2}$ in the earlier example because at each reassignment, we related $\textrm{fold}_{\sigma_0}$ to $\textrm{fold}_{\sigma_1}$ and $\textrm{fold}_{\sigma_1}$ to $\textrm{fold}_{\sigma_2}$, so there is a path between the start and end. Relating $\textrm{fold}_{\sigma_0}$ to $\textrm{fold}_{\sigma_1}$ amounts to decomposing each fold by removing index $i$ for their respective filters, which allows deducing $\textrm{fold}_{\sigma_1} = \textrm{fold}_{\sigma_0} + 1$. Likewise, for the next reassignment, we decompose each fold $\textrm{fold}_{\sigma_1}$ and $\textrm{fold}_{\sigma_2}$ by removing index $j$ from their respective filters, which allows deducing $\textrm{fold}_{\sigma_2} = \textrm{fold}_{\sigma_1} + 1$. 


So, the rules of this modular strategy are as follows:
\begin{enumerate}
    \item Each heap-modifying operation is a node
    \item Each (primary) fold in the heap state before the heap-modifying operation is a directed edge into the node, a  \emph{fold input}. For example, if a statement "A" mutates the heap from $\sigma_i$ to $\sigma_j$, then $\textrm{fold}_{\sigma_i}$ is an edge into the node.
    \item Each heap-modifying operation generates a new intermediate fold, considered a directed edge out of the node, or a \emph{fold output}. This fold is also considered primary. An exception is if the user writes the fold directly at the heap state $\sigma_j$. Then, this is not an out-edge, as it is not an intermediate fold generated by our strategy.
    \item The folds in and folds out are both decomposed with some node-specific rule. This allows us to establish some relation between the folds in and out. The decomposed folds with smaller filters are secondary folds, written as $\textrm{fold}'$. Secondary folds do not get decomposed further.
\end{enumerate}

We will employ this strategy for the rest of the thesis. Next, we extend it further to handle "exhale" and "inhale", the two other heap-modifying operations. The challenge will be defining exactly what goes in and out of these two nodes.

So far, the strategy for each heap-modifying node are:
\begin{enumerate}
    \item Reassignment $r(i).F := ...$, mutating heap from $\sigma_i$ to $\sigma_j$: 
    If fold input is $\textrm{fold}_{\sigma_i}(f)$, then fold output is $\textrm{fold}_{\sigma_j}(f)$. 
    Decompose by removing $i$ from both folds.
    \item Exhale: ...
    \item Inhale: ...
\end{enumerate}

\subsection{Example Combining Exhales and Inhales}

Before we explain the handling of inhale and exhale statements, consider the following example. Suppose we have a filter $f$ of type "Set[Int]" and a subset $f_1 \subset f$. Suppose also we have an integer $a$ in $f$ but is not part of $f_1$, \ie, $a \in f \land a \not\in f_1$. Then, consider the next lines of code.
\begin{lstlisting}
method exhale_reassign_inhale() {
    ...
    // Exhale, changes heap state from 0 to 1
    exhale forall i: Int :: i in f1 => acc(r(i).val)
    // Reassignment, changes heap state from 1 to 2
    r(a).val := r(a).val + 1
    // Inhale and assume nothing changed
    // , changes heap state from 2 to 3
    inhale forall i: Int :: i in f1 => 
            acc(r(i).val) && r(i).val == old(r(i).val)

    // Pseudocode asserting change in the fold
    assert fold[sum](f) = old(fold[sum](f)) + 1 
}
\end{lstlisting}
The assertion at the end aims to prove 
$$\textrm{fold}_{\sigma_{3}}(f) = \textrm{fold}_{\sigma_{0}}(f) + 1.$$ Note how the users only explicitly mention heap states $\sigma_{0}$ and $\sigma_{3}$, \ie, the heap states at the start and end. All other heap states are considered intermediate.

Following our local and intermediate decomposition plan, we need a fold in the state right before each heap modifying operation. So, for the reassignment, we need a fold input $\textrm{fold}_{\sigma_{1}}(f)$ to generate fold output $\textrm{fold}_{\sigma_{2}}(f)$. The input fold should be generated as an output fold of the exhale statement. However, things get tricky because $\textrm{fold}_{\sigma_{1}}(f)$ might not even be well-defined with regard to permissions. 

\section{Exhale Case}
The second way that the heap could be modified is through an exhale statement. An exhale statement containing an accessibility predicate "acc(A)" would result in losing the permissions to "A." In addition, an exhale statement could also contain quantified permissions, which would mean losing an unbounded amount of permissions to the locations under the quantifier. 

Consider the statement 
\begin{lstlisting}
{
    exhale P
}
\end{lstlisting}
where "P" may contain accessibility predicates and quantified permissions. Let the heap immediately prior to this statement be $\sigma_0$, and the heap immediately after is $\sigma_1$. Suppose, we are interested in the folds $\textrm{fold}_{\sigma_0}[o]( m(r.F) \mid f)$ with the addition operator $o$ and the identity mapping $m$. After the exhale, we want to know how it evolves into $\textrm{fold}_{\sigma_1}[o]( m(r.F) \mid f)$.

\subsection{Well-definedness Problem After Losing Permissions}
Before we can even discuss these folds, there is a potential well-definedness problem. Even if $\textrm{fold}_{\sigma_0}[o]( m(r.F) \mid f)$ is well-defined, the second fold might not be. $\sigma_1$ may not have all the permissions to all the heap locations defined by filter $f$, receiver $r$, and field $F$, because the exhale might have given permissions away. For example, suppose the statement is "exhale acc(r(i).val)" to some $i \in f$. Then, by the semantics of exhales, the heap $\sigma_1$ does not contain permissions to key "(r(i), val)", and $\textrm{fold}_{\sigma_1}[o]( m(r.F) \mid f)$ would be ill-defined!

\subsection{Modular Design to Handle Exhales}
Remember that we still want to consider each heap modification modularly. As discussed in the previous section, we consider a heap-modifying line of code as a node, and for every fold coming in, there must be a fold going out. Additionally, the node should perform a decomposition on each fold, both in and out, to establish a relationship between them. Altogether, this allows us to link two folds from the start of the method to the end. So, if $\textrm{fold}_{\sigma_0}[o]( m(r.F) \mid f)$ enters the "exhale" node, what should come out? And what decompositions does the node do to the folds?

To answer the first question, we need to define a new $f'$ such that $\textrm{fold}_{\sigma_1}(f')$ is well-defined. We can do that by removing all the heap locations without permission as a result of the exhale. 

\subsubsection{Defining a Well-Defined Output Fold}
Let $\sigma_0.\textrm{keys}(F)$ represent the set of "Ref" keys in heap $\sigma_0$ with some field $F$. Recall that we represent the heap as a partial map typed \lstinline{(Ref, Field A) $\rightarrow$ A}, which has tuple keys of the form "(Ref, Field A)". So, think of this $\textrm{keys}(F)$ function as extracting the tuple keys from the heap, filtering the tuple such that the second element is equal to $F$, and outputting the first element typed "Ref" of the remaining tuples. Recall that each key on $\sigma_0$ exists if and only if the method has permissions to that heap location at the program point.

Then, we define 
\begin{definition}[Lost References]\label{def:Lost}
$$L := \sigma_0.\textrm{keys}(F) - \sigma_1.\textrm{keys}(F),$$
\ie, let $L$ be the set of references that the exhale gave away permissions to, with respect to field $F$.
\end{definition}

Then, we define the filter of remaining elements $f'$ as
\begin{definition}[Filter Not Lost]\label{def:notLost}
$$f' := \{e \in f | r(e) \not\in L \},$$ where $r$ is the receiver function. 
\end{definition}
By definition, $f'$ guarantees that $\textrm{fold}_{\sigma_1}(f')$ is well-defined with regards to permissions. 

\subsubsection{Decomposition of In and Out Folds}
Next, we decide the decomposition of the folds in and out of the exhale node. For the fold input, \ie, $\textrm{fold}_{\sigma_0}(f)$,  there is one obvious contender. We can decompose $f$ using $f'$! By definition,  $f'$ is a subset of $f$, which gives us the decomposition $f = f' \uplus (f-f')$. Applying this to the fold evaluation, we get the fold decomposition
$$\textrm{fold}_{\sigma_0}(f) = \textrm{fold}'_{\sigma_0}(f') \oplus \textrm{fold}'_{\sigma_0}(f - f').$$ Keep in mind that the folds on the RHS of that equation are secondary folds, so they do not get decomposed any further and are not considered as a directed edge for future heap-modifying nodes.

For the fold output $\textrm{fold}_{\sigma_1}(f')$, there is no decomposition needed. By permission framing, we know that $\textrm{fold}_{\sigma_1}(f') = \textrm{fold}'_{\sigma_0}(f')$ because the method holds permissions to all elements defined by $f'$ across the "exhale" statement, and the exhale will not have modified any of those elements unlike the explicit reassignment ":=". However, by explicitly declaring $\textrm{fold}_{\sigma_1}(f')$ as a fold output, we have elevated it from being a secondary fold in $\textrm{fold}'_{\sigma_0}(f')$ to a primary fold $\textrm{fold}_{\sigma_1}(f')$. That means the filter $f'$ could become a candidate for decomposition in future heaps.

So, we update the strategy for heap-modifying nodes.
\begin{enumerate}
    \item Reassignment $r(i).F := ...$, mutating heap from $\sigma_i$ to $\sigma_j$: 
    If fold input is $\textrm{fold}_{\sigma_i}(f)$, then fold output is $\textrm{fold}_{\sigma_j}(f)$. 
    Decompose by removing $i$ from both folds.
    \item Exhale, mutating heap from $\sigma_i$ to $\sigma_j$ via permission loss. If fold input is $\textrm{fold}_{\sigma_i}(f)$, then fold output is $\textrm{fold}_{\sigma_j}(f')$ where $f'$ is the subset of $f$ where permissions were not lost. Decompose by subtracting $f'$ from $f$ for the fold inputs. Fold outputs do not need a decomposition.
    \item Inhale: ...
\end{enumerate}

This strategy creates the flow of folds as part of the modular strategy design. Should there be a heap assignment after an exhale statement, the exhale strategy outputs a fold as input to the corresponding node. For example,
\begin{lstlisting}
    exhale acc(r(i).val)
    r(j).val := r(j).val + 5
\end{lstlisting}
This code snippet has two heap-modifying nodes. If the fold input to the first "exhale" node is $\textrm{fold}_{\sigma_0}(f)$, then the fold output to the same node is $\textrm{fold}_{\sigma_1}(f')$, which itself is a fold input to the ":=" node.

In addition to continuing the data flow, this exhale strategy can also prove some interesting examples in isolation. Suppose we have an arbitrary subset $f'' \subseteq f$. Then, suppose the user assumes, through some assume statement or requires clause, $\textrm{fold}_{\sigma_0}[\textit{sum}](f'') = 7$ and $\textrm{fold}_{\sigma_0}[\textit{sum}](f) = 10$. Then, there is the statement 
"exhale forall i: A :: i in f'' => acc(r(i).F)", which translates to exhaling all elements of filter $f''$. 

The user now wants to prove $\textrm{fold}_{\sigma_1}[\textit{sum}](f - f'') = 3$ in an assert statement. This works because from our strategy, we will have generated the equations
$$\textrm{fold}_{\sigma_0}[\textit{sum}](f) = \textrm{fold}'_{\sigma_0}[\textit{sum}](f') + \textrm{fold}'_{\sigma_0}[\textit{sum}](f - f'), $$ and
$$\textrm{fold}_{\sigma_1}[\textit{sum}](f') = \textrm{fold}'_{\sigma_0}[\textit{sum}](f'). $$
We also know that $f' = f - f''$ and $f - f' = f''$. From equational reasoning, we deduce
\begin{align*} 
\textrm{fold}_{\sigma_0}[\textit{sum}](f) &= \textrm{fold}'_{\sigma_0}[\textit{sum}](f - f'') + \textrm{fold}'_{\sigma_0}[\textit{sum}](f'') \\
10 &= \textrm{fold}'_{\sigma_0}[\textit{sum}](f - f'') + 7 \\
\textrm{fold}'_{\sigma_0}[\textit{sum}](f - f'') &= \textrm{fold}_{\sigma_1}[\textit{sum}](f - f'') = 3. 
\end{align*}
In practice, however, the equalities $f' = f - f''$ and $f - f' = f''$ are not so obvious to the SMT solver, and they require some explicit extensional equality checks. We will discuss this more in a future chapter.

The exhale strategy allows for reasoning through its decomposition, while also generating intermediate folds as inputs for further heap mutation nodes.

\section{Inhale Case}
Now, we consider the last scenario of heap mutation, the inhale statement, which may add permissions and new keys to the heap $\sigma$. 

Intuitively, an inhale is the opposite of an exhale. In our exhale strategy, we start with input fold $\textrm{fold}_{\sigma_{in}}(f)$, lose some permissions, construct a new filter $f' \subseteq f$ where permissions are retained, decompose the initial fold by subtracting the new filter $f'$, and finally, generate an intermediate fold out $\textrm{fold}_{\sigma_{out}}(f')$. 

In an inhale strategy, we could simply do it the reversed way. We could start with some input fold $\textrm{fold}_{\sigma_{in}}(f)$, gain some permissions, find a larger filter $f^* \supseteq f$ where $f^*$ includes items from the gained permissions, generate an output fold $\textrm{fold}_{\sigma_{out}}(f^*)$, and then decompose this new output fold by subtracting the input filter $f$.

There are two main questions regarding this.
\begin{enumerate}
    \item How do we know what $f^*$ to construct?
    \item Is this enough?
\end{enumerate}

\subsection{Finding the Output Fold Filter}
Picking which $f^*$ to construct from a smaller filter $f$ is not so obvious. One viable idea is to construct the set of all heap locations gained from the inhale, extract a filter set out of that, and union it to $f$. 

However, this idea is problematic when the amount of permissions gained exceeds the filter of interest. For example, consider the following snippet.
\begin{lstlisting}
method test()
    requires forall i: Int:: i in f => acc(r(i).F)
{
    exhale forall i: Int:: i in f_1 => acc(r(i).F)
    label a
    
    assume e in (f - f1)
    r(e).F := r(e).F + 1
    label b
    
    inhale forall i: Int:: i in f_1 || i in f_2 => acc(r(i).F) 
    inhale forall i: Int:: i in f_1 => r(i).F == old(r(i.F))
            
    label c
    assert fold[sum](f) = old(fold[sum](f) + 1)

}
\end{lstlisting}
Suppose that $r$ is an injective receiver, and the filters $f$ and $f_1 \subset f$. Also, suppose $f_2$ is disjoint from filter $f$. So, in the code snippet, we start with permission to elements defined by filter $f$. Then, an exhale happens, removing permissions to filter $f_1$. Then, a reassignment happens to some arbitrary heap location defined by $e \in (f - f_1)$. Then, an inhale happens, where the method gets permissions to elements defined by filters $f_1$ and $f_2$. Another inhale assumes that all the heap elements defined by $f_1$ remain unchanged throughout this whole process (framing axiom). We now want to prove that 
$$\textrm{fold}_{\sigma_{c}}(f) = \textrm{fold}_{\sigma_{old}}(f) + 1$$

If we use the modular strategy, we can think of folds as inputs and outputs to heap modifying nodes. In our example, the first exhale node has input $\textrm{fold}_{\sigma_{old}}(f)$, and output fold $\textrm{fold}_{\sigma_{a}}(f')$ where $f' = f - f_1$. Then, the reassignment node has input 
$\textrm{fold}_{\sigma_{a}}(f')$ and output $\textrm{fold}_{\sigma_{b}}(f')$.
The node that inhales access then has input $\textrm{fold}_{\sigma_{b}}(f')$. If we simply take all the permissions and put it in a filter, the inhale node would have output $\textrm{fold}_{\sigma_{c}}(f \cup f_2 ).$

However, for that final assert, we need some knowledge about $\textrm{fold}_{\sigma_{c}}(f)$, but we do not with this strategy! Instead, we have $\textrm{fold}_{\sigma_{c}}(f \cup f_2 ),$ which has a bigger filter than we want.

A better idea is to construct the filters that were there before the exhale. If we start with a fold on some filter $f$, lose some permissions to it, and then gain permissions back, it makes sense to construct $f$ again. Concretely, if we start from some fold before an exhale, we should aim to construct the same fold after an inhale. To decide the output fold of an inhale node, we look back at the input folds of an exhale node. In our case, the exhale node has input fold $\textrm{fold}_{\sigma_{old}}(f)$, with filter $f$. Then, at the inhale node, we can output the fold with the same filter $f$.

To explain the strategy in more detail:
\begin{enumerate}
    \item Look at the input folds to all previous exhales. Extract the filters.
    \item Check that the inhale has retrieved new permissions to the filter for well-definedness. 
    \item Check that the filter is new, \ie, the method did not have full permissions to the filter before the inhale. 
    \item For each extracted filter $f^*$, construct $\textrm{fold}_{\sigma_{new}}(f^*)$ as an output fold to the inhale node.
\end{enumerate}

Then, similar to the exhale strategy, we decompose this output fold $\textrm{fold}_{\sigma_{new}}(f^*)$ by subtracting the filter $f$, which is part of the input fold. At the exhale node, the input fold $\textrm{fold}_{\sigma_{old}}(f^*)$ should also be decomposed by subtracting the filter $f$ for symmetry. 

So, the final strategies for heap-modifying nodes are
\begin{definition}[Final Strategies] \label{strat:final}
There are the final strategies for each heap-modifying statement.
\begin{enumerate} 
    \item Reassignment $r(i).F := ...$, mutating heap from $\sigma_i$ to $\sigma_j$: 
    If input fold is $\textrm{fold}_{\sigma_i}(f)$, then output fold is $\textrm{fold}_{\sigma_j}(f)$. 
    Decompose by removing $i$ from both folds.
    \item Exhale, mutating heap from $\sigma_i$ to $\sigma_j$ via permission loss. If input fold is $\textrm{fold}_{\sigma_i}(f)$, then output fold is $\textrm{fold}_{\sigma_j}(f')$ where $f'$ is the subset of $f$ where permissions were not lost. Decompose by subtracting $f'$ from $f$ for the fold inputs. Fold outputs do not need a decomposition.
    \item Inhale: mutating heap from $\sigma_i$ to $\sigma_j$ via permission gain. If the input fold is $\textrm{fold}_{\sigma_i}(f)$, search the input folds to previous exhales for a filter $f^* \supset f$. If new heap state has permissions to $f^*$, construct the output fold $\textrm{fold}_{\sigma_j}(f^*)$. Decompose the output fold by subtracting $f$ from $f^*$. Fold inputs do not need a decomposition.
\end{enumerate}
\end{definition}

\subsection{Folds Flowing Backwards with Lookahead}
There are certain limitations to this inhale strategy. If there are inhale statements without prior exhales, then our strategy does not generate any output fold. We become reliant on the user writing the fold manually. 

Consider the following example, with $f$ and $f''$ as disjoint filters. 
\begin{lstlisting}
{
    assume fold[sum](f) == 3
    inhale forall i: A :: i in f'' => acc(r(i).F)
    assume fold[sum](f') == 7
    assert fold[sum](f union f'') == 10
}
\end{lstlisting}

Here, the inhale node does not generate any new folds because there is no prior exhale statement. At the point of the inhale, no input fold has any information about the filter "f union f''", so the assert would not verify. 

An approach we considered to solve this is to use some lookahead. Our inhale strategy looks back at input folds to previous exhales, so why not look forward at future relevant folds? Perhaps those future folds can be fed as input folds, like data flowing backwards. In the mentioned example, if the inhale node could somehow see $$\textrm{fold}_{\sigma}(f \cup f'')$$ at the end of the method, then we can perform the decomposition and generate an output fold in the previous heap state. 

If this is possible, we can treat an inhale exactly like an exhale; it just needs to be read backwards. At the line before an exhale statement, the method has permissions and maybe some fold with a filter $f$. At the line after, the method has fewer permissions and a fold with a smaller filter $f'$. For inhales, it is the same read backwards. At the line after an inhale, the method has permissions and maybe some fold with a filter $f$. At the line before, the method has fewer permissions and a fold with a smaller filter $f'$. If we allow data to flow forward and backward, we can treat inhales and exhales in the same way.

We are unable to do this due to practical limitations. As we will explain in the upcoming chapter, generating intermediate folds from folds in a previous heap state relies crucially on quantifier triggering. Folds in the previous heap state generate folds in the following heap state. If folds in the following heap state also generate folds in the previous heap state, we potentially end up in a matching loop. New generates old, which generates new, which generates old, \etc

The other practical limitation comes from lookahead triggering. Our experiments show that including state-dependent expression in quantifier triggers has some undocumented behaviour, especially with triggers containing terms from a future state. The triggers simply do not work as you would expect, so we could not achieve this look-ahead idea.

\chapter{Modelling in Viper}
Now that we have explained the strategies for supporting folds formally, in this chapter, we will discuss encoding them into Viper.

\section{Heap-dependent Fold Definition In Viper}
Consider our fold notation as defined in the previous chapter, 
$$\textrm{fold}_{\sigma}[o]( m(r.F) \mid f),$$
with filter $f$, receiver $r$, and field $F$, where heap $\sigma$ is the current heap, and operator $o$ is the operator object containing a binary operation $(B, B) \rightarrow B$ and an identity element $i_0 : A$ for that operation.

To translate this into Viper, there are a few challenges. Firstly, Viper does not allow first-class functions. Viper supports first-order logic, so we cannot quantify over functions, and function arguments cannot be functions. The second challenge comes from the limitations in interacting with the heap. Although the heap is part of the Viper program state, the heap is not an expression. Like functions, the heap is not a valid function argument and cannot be a quantified variable. 

\subsection{Defunctionalization}
To solve the first challenge, we first apply defunctionalization to convert each input function to a combination of domain declarations. First, we declare the domain types for each of the function arguments to a fold. We declare "Receiver[A]", "Operator[B]", "Mapping[V,B]" as domain types with type arguments "A", "V", and "B". The filters can be represented with a Viper's built-in "Set", so we do not define new types.

For each of these domain types, we define an "apply" function that takes the newly defined types as the first argument. For example, the receiver domain has a function "recApply(r: Receiver[A], a: A) : Ref". Then, we declare axioms to describe the results of these functions. These axioms depend on the function definitions written by the user. If the axioms refer to a specific instance, like "Receiver[Int]" with type variable "A" replaced by "Int", it can even be written in a different domain. For now, here is the "Receiver" domain declaration with the axiom
\begin{lstlisting}
domain Receiver[A]  {
  function recApply(r: Receiver[A], a: A): Ref 
  function recInv(rec: Receiver[A], ref: Ref): A 
}
\end{lstlisting}
The receiver inverse "recInv" is something we get from injectivity (discussed later in the well-definedness checks section). A receiver maps an element of a filter to a reference. Sometimes, given an arbitrary reference, we want to extract the original element from the filter.  We do so using the "recInv". For example, if there is a reassignment "r(i).val := ...", we need to be able to extract "i" with "recInv(r(i))" to perform the appropriate fold decomposition.

The "Operator[B]" domain has an "opApply" function that takes two additional arguments, each representing an input to a binary operator. In addition, each operator should have an identity value, so we define another function, "opGetIden(o: Operator[B]): B," that outputs the specified identity.
\begin{lstlisting}
domain Operator[B]  {
  function opApply(op: Operator[B], val1: B, val2: B): B 
  function opGetIden(op: Operator[B]): B 
}
\end{lstlisting}

Likewise, the "Mapping[V,B]" domain has a "mapApply" function for applying the mapping.
\begin{lstlisting}
domain Mapping[V, B]  {
  function mapApply(m: Mapping[V, B], _mInput: V): B 
}
\end{lstlisting}

This is similar to classes in object-oriented programming languages like Python. Each domain type is a class, and the "apply" functions are methods that each takes "self" as the first argument. 

\subsection{Fold Object}
Like the other types we have defined, we can also defunctionalize the fold function, representing it as a domain type or a class. From a class point of view, we define fold as a class that takes three constructor arguments: a receiver "Receiver[A]", a mapping object "Mapping[V,B]", and an operator "Operator[B]". Unfortunately, "fold" is already a keyword in Viper for folding predicates, so we use "hfold" for heap-dependent fold as the constructor instead.
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
domain Fold[A,V,B] {
  function hfold(r: Receiver[A], m: Mapping[V,B], op: Operator[B]): Fold[A,V,B]
}
\end{lstlisting}

Given a "hfold" object, we should be able to extract the components from it, namely the receiver, operator, and mapping. We declare getter functions for them.
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
domain Fold[A,V,B] {
  function hfold(r: Receiver[A], m: Mapping[V,B], op: Operator[B]): Fold[A,V,B]
  
  function getreceiver(c: Fold[A, V, B]): Receiver[A] 
  function getoperator(c: Fold[A, V, B]): Operator[B] 
  function getmapping(c: Fold[A, V, B]): Mapping[V, B] 
}
\end{lstlisting}
Now, there are still some arguments missing from this encoding, namely the filter, the field, and the heap. These will appear as arguments to another function that generates the snapshot.

\subsection{Snapshot Introduction}
As we explained earlier, a fold evaluation only depends on a portion of the heap defined by the filter, the receiver, and the field. We refer to this portion as a snapshot of the heap, which we have defined as $$\textrm{snap}^{r.F}_{f}(\sigma) = \{ e \rightarrow \textrm{value} \mid ([r(e), F] \rightarrow \textrm{value}) \in \sigma \land e \in f\}.$$

In our Viper encoding, we modify the values in the snapshot map to include the application of the mapping function. As written earlier, an alternative representation of a fold using a snapshot is $$\textrm{fold}[o]\{m(\textrm{snap}^{r.F}_{f}(\sigma)\}.$$ Notice the $m$ applied to the snap. In our Viper encoding, we decide to distribute that $m$ application into the values of the snap map, which we can express as $$\textrm{fold}[o]\{\textrm{snap}^{m(r.F)}_{f}(\sigma)\}.$$

Recall that the keys to this snapshot map should be an element of the filter, \ie, they have type "A". Applying the receiver $r$ to this "A" should give a "Ref", which would be a key to the heap map. Also, the snapshot function takes a field argument, which limits the keys of the heap map only to the tuple with the specified field.

A first attempt in pseudocode at encoding this snap function might look like the following. 
\begin{lstlisting}
domain Fold[A,V,B] {
  function hfold(r: Receiver[A], m: Mapping[V,B], op: Operator[B]): Fold[A,V,B]

  function snap(fo: Fold[A,V,B], filter: Set[A], 
                field: V, heap: Map[(Ref, Field V), V]): Map[A,B]
}
\end{lstlisting}
Since the snapshot function depends on the receiver and mapping function, our function takes a "Fold" object that already encapsulates a "Receiver" and a "Mapping". 

The snap function above is pseudocode because of a few practical limitations. For one, the field and the heap cannot be included as function arguments. More importantly, the snap function is a heap-dependent function, which cannot be declared inside a Viper domain definition. 

Functions defined outside a domain declaration in Viper take the heap as an implicit argument. However, these non-domain functions cannot utilize type arguments, so the argument "fo: Fold[A,V,B]" would not be valid. So in practice, an actual snap function would look like the following.
\begin{lstlisting}
function snap_Int_Int_Int_val(fo: Fold[Int, Int, Int], filter: Set[Int]): 
    Map[Int,Int]
    requires forall i: Int :: {i in filter}  i in filter ==> 
        acc(recApply(getreceiver(fo), i).val)

    ensures domain(result) == filter
    ensures  (forall i: Int :: { result[i] }
      (i in filter) ==> result[i] ==
       (mapApply((getmapping(fo)), 
                 (recApply((getreceiver(fo)), i)).val)))
    ensures getSnapFieldID(result) == 1
\end{lstlisting}
This snap function is intended for a "Fold[Int, Int, Int]" instance and the "val" field. The field "val" in this example is inlined into the precondition, so it is not an argument. Altogether, this means that we must define a separate snap function declaration for each "Fold[A,V,B]" and each field. Because Viper does not allow type arguments for heap-dependent functions, we need to perform monomorphization manually in our plugin; this amounts to declaring a new function for each instance of the type variables "A, V, B" and the relevant field. 

Note that the preconditions contain quantified permissions, denoting access to all indices defined by "receiver(i).val," where "i" is an element of the filter. To create a snapshot of the heap elements, we need permission to read the heap elements. Technically, this can be fractional permissions since we only need read access. Also, using quantified permissions requires injectivity of the receiver expression with respect to the filter. This is a well-definedness check that we will include as another function precondition. We discuss this in the upcoming Well-definedness Checks section. 

For now, we omitted some other specifications of the snap functions; there is an injectivity check of the receiver and post-conditions to aid with fold decomposition, filter equality checks, and connection to a secondary snap function, "snap_prime." We will explain these things gradually.

The ensures clause of the snap function defines the output map. The keyword "result" in Viper denotes the output of a function, and function postconditions typically refer to the "result" extensively. The first post-condition above ensures that the output's keys, also called domain, are equal to the filter set. The second post-condition ensures that the output map's value for key "i" is equal to "mapping(receiver(i).val)" with the appropriate defunctionalized apply functions.

With this snap function defined, we can declare an "apply" function for "Fold", with inputs of the fold object and the snapshot output.
\begin{lstlisting}
domain Fold[A,V,B] {
  function hfold(r: Receiver[A], m: Mapping[V,B], op: Operator[B]): Fold[A,V,B]

  function hfoldApply(fo: Fold[A,V,B], snap: Map[A,B]): B

  function hfoldApply1(fo: Fold[A,V,B], snap: Map[A,B]): B
}
\end{lstlisting}
This mimics the earlier definition,
$$\textrm{fold}[o]\{\textrm{snap}^{m(r.F)}_{f}(\sigma)\}.$$ The "hfoldApply1" is the secondary foldApply, as denoted with $\textrm{fold}'$. Secondary foldApply does not get decomposed, preventing matching loops. 

Notice that despite the snap function being heap-dependent, its map output is considered pure, allowing us to use it as an argument "snap: Map[A,B]" to a pure domain function "hfoldApply". Fortunately, this means we can use type arguments and declare a polymorphic "hfoldApply". Given the choice, we generally prefer to define functions as pure domain functions instead of heap-dependent ones. 

\subsection{Encoding FoldApply}
Now that we have defined an apply function for fold, we need to write axioms to describe the function's behaviour. 

As described in the previous chapter, a heap-dependent fold is defined recursively, split into 3 different cases. The first case is when the input filter is empty, the fold should output the identity value, defined by the operator.
\begin{lstlisting}
axiom _emptyFold {
    (forall c: Fold[A, V, B], snap: Map[A,B] ::
      { (hfoldApply(c, snap)) }
      domain(snap) == Set[A]() ==>
        (hfoldApply(c, snap)) == 
          (opGetIden((getoperator(c))))
  }
\end{lstlisting}
The trigger for this axiom is just an instance of a fold application.

The second case is when the input filter is a singleton set. The output should be the mapping function applied to an element on the heap. In our case, the mapping function is already included in the snapshot map values, so we simply need to read the snapshot at key equal to the single element in the filter.
\begin{lstlisting}
axiom _singleton {
  (forall c: Fold[A, V, B], snap: Map[A,B], elem: A ::
      { (hfoldApply(c, snap)), Set(elem) }
      domain(snap) == Set(elem) ==>
        (hfoldApply(c, snap)) == snap[elem])
}
\end{lstlisting}
The trigger is the fold application and a mention of a singleton set "Set(elem)". 

The third case is the decomposition case. Recall that we perform two kinds of decompositions in our strategies for handling inhales, exhales, and heap reassignment. For inhales and exhales, start from a filter "f", and then set-minus a subset "f'". For heap reassignment, we subtract a singleton set from the filter "f". We distinguish between these two cases into two scenarios: subtracting a singleton filter or subtracting a whole filter.

\subsubsection{Singleton Decomposition Axiom}
The case for singleton decomposition can be axiomatized by the following domain axiom:
\begin{lstlisting}[label=axm:drpone,caption=Decomposition By Removing 1 Element]
axiom _dropOne {
forall fo: Fold[A, V, B], snap: Map[A,B], key: A ::
{ triggerDeleteKey((hfoldApply(fo, snap)), key) }
(key in domain(snap)) ==>
hfoldApply(fo, snap) == 
  opApply(getoperator(fo), 
    hfoldApply1(fo, mapDelete(snap, Set(key))), 
    snap[key])
}
\end{lstlisting}
In other words, if the element "key" is in the filter, then "hfold(snap) = op(hfold1(snap - key), snap[key])" (omitting the defuntionalization for brevity). There are a few things to observe with this axiom. Firstly, the trigger has a function "triggerDeleteKey(...)", which we have not discussed. This is simply a dummy boolean function used to trigger the axiom when necessary. The plugin will assume an instance of this dummy trigger function when we intend to perform a decomposition, as described in the reassignment strategy in the last chapter.

The other interesting point is the operations on the snap function. As described earlier, "domain(snap)" refers to the filter. In the the previous chapter, a singleton decomposition involves removing a key from the filter, which we encoded here as "mapDelete(snap, Set(key))". Viper's built-in "Map" type does not have a function that deletes keys, so our plugin generates those manually. For now, think of "mapDelete" as deleting a set of keys from the snapshot, which is equivalent to removing keys from the filters. We will discuss further details about the snapshot function later.

This axiom generates only one new secondary fold. The second fold from the decomposition is a fold with a singleton filter, so we inline its evaluation, \ie, "snap[key]" explicitly.

\subsubsection{Set Decomposition Axiom}
The next type of decomposition involves subtracting a whole filter.
\begin{lstlisting}[label=loseMany, caption:Domain Axiom for General Decomposition]
axiom loseMany {
forall c: Fold[A, V, B], snap1: Map[A,B], keys: Set[A] ::
{ triggerDeleteBlock((hfoldApply(c, snap1)), keys) }
(keys subset domain(snap1)) ==>
  (hfoldApply(c, snap1)) ==
  opApply(getoperator(c), 
    hfoldApply1(c, mapDelete(snap1, keys)),
    hfoldApply1(c, mapSubmap(snap1, keys)))
}
\end{lstlisting}
The axiom states that if filter "keys" is a subset of the current filter ,\ie, the domain of the snapshot, then the fold can be decomposed as "hfold(snap) = op(hfold1(snap - keys), hfold1(snap$\upharpoonright$keys))". The notation "snap$\upharpoonright$keys" refers to a projection of the snapshot map onto the specified keys, \ie, restricting the map onto the filter "keys". Viper builtin Map type lacks this operation, so we generate another function, "mapSubmap", to model this operation. 

Notice again that this axiom relies on another dummy boolean function, "triggerDeleteBlock." The plugin will generate an instance of the triggering function when implementing the inhale and exhale strategies. 

\subsection{Example: Array Encoding}
We present an example encoding an array sum, \ie, its receiver, mapping, operator, \etc Recall that the standard array encoding uses a field and an "Array" domain. We will use an array of integers, so we declare a field of type "Int".
\begin{lstlisting}[language=silver,numbers=left, firstnumber=1, stepnumber=1]
field val: Int

domain Array {
  function loc(a: Array, i: Int): Ref
  function len(a: Array): Int

  axiom len_nonneg {
    forall a: Array :: { len(a) }
      len(a) >= 0
  }

  // functions and axioms to ensure loc() injectivity
  function first(r: Ref): Array
  function second(r: Ref): Int  

  axiom all_diff {
    forall a: Array, i: Int :: {loc(a,i)}
      first(loc(a,i)) == a && second(loc(a,i)) == i
  }
}
\end{lstlisting}
The last "all_diff" axiom ensures that the "loc" function has inverses "first" and "second". This also implies that "loc", the Array receiver function, is injective. 

For an array, the receiver function should be from an integer index ("Int") to a "Ref". So, declare a function to generate a "Receiver[Int]" object, and an axiom describing the "recApply" function on this object.
\begin{lstlisting}
function arrayRec(a: Array): Receiver[Int] 

axiom {
(forall a: Array, i: Int ::
    { (recApply(arrayRec(a), i)) }
    { loc(a, i) }
    recApply(arrayRec(a), i) == loc(a, i))
}
\end{lstlisting}
The axiom relates the "Receiver[Int]"'s application to the Array domain's "loc" function. One thing to note is that the "loc" function takes two arguments: an array and an index. But a receiver function, by definition, only takes one input. We circumvent this issue by having "arrayRec" take an array argument to output a "Receiver[Int]" instance. That array argument can now be considered like a field in the object and can be used in the axiom. So technically, "recApply" still only takes one argument in addition to self, \ie, "arrayRec(a)".

The triggers for the axioms are chosen to be "recApply(arrayRec(a), i)" or "loc(a,i)". Triggers written in two curly braces are considered disjunction triggers; only one must match to instantiate the axiom. We have added both the LHS and RHS of the equations to ensure bidirectional triggering. Bidirectional triggering is a heuristic for choosing a trigger by ensuring that a universal quantifier involving an implication or equality can be triggered both by the left-hand terms or the right-hand terms, improving completeness. More discussion on picking triggers will come in the next sections.

Next, we create a mapping function instance, \ie, "Mapping[Int,Int]". For an array sum, this should just be the identity function. We can define a general identity mapping function because it will be quite common.
\begin{lstlisting}
function mapIdentity(): Mapping[V, V] 
  
axiom {
(forall v: V ::
{ (mapApply((mapIdentity()), v)) }
    (mapApply((mapIdentity()), v)) == v)
}
\end{lstlisting}
Applying the identity mapping function changes nothing to the input "v".

Next, we define the addition operator, "Operater[Int]", which should also have 0 as the identity.
\begin{lstlisting}
function add(): Operator[Int] 
  
axiom {
(forall a: Int, b: Int ::
  { opApply(add(), a, b) }
  opApply(add(), a, b) == a + b)
}
axiom {
  opGetIden(add()) == 0
}
\end{lstlisting}
The first axiom defines the "apply" function for the "add()" instance of "Operator[Int]"; since it is a binary operator, the apply function takes two additional arguments. Notice that the trigger only contains the LHS of the equation, because "a + b" is not a valid trigger. SMT solvers typically disallow interpreted symbols like plus "+" or less than "<" from triggers, so "a + b" would not be allowed. For practical use cases, this axiom being less complete does not make a significant difference.

The second axiom defines the identity of addition, 0. The axiom does not use a quantifier, but it is could if the instance "add" contains any arguments. 

Lastly, we define a filter, which would be a set of indices. Typically, one would specify the array sum over a range of indices starting from "start" to "end," exclusive of the end.
\begin{lstlisting}
function intRange(start: Int, end: Int): Set[Int] 

axiom {
(forall start: Int, end: Int, i: Int ::
  { (i in intRange(start, end)) }
  (i in intRange(start, end)) == (i >= start && i < end))
}
\end{lstlisting}
Filters can be defined as a set represented by a Boolean expression, like "i >= start && i < end". Any index "i" that makes the expression true would be considered part of the set. Similar to the operator axiom, the filter axiom triggers only on the LHS of the equation because ">=, &&, <" on the RHS are all interpreted symbols forbidden in triggers. 

Together with the "snap_Int_Int_Int_val" declaration of the snapshot function, we can declare a fold object and the fold application. Suppose we want to write the sum of the array from indices 0 up to the array length. Given an array "a", the fold instance can be instantiated with "hfold(arrayRec(a), mapIdentity(), add())", and a fold application is 
\begin{lstlisting}
hfoldApply(hfold(arrayRec(a), mapIdentity(), add()),
    snap_Int_Int_Int_val(
        hfold(arrayRec(a), mapIdentity(), add()), 
        intRange(0, len(a)))).
\end{lstlisting}
The fold object "hfold(arrayRec(a), mapIdentity(), add())" is repeated, but this does not cause any overhead because it is not really an object. No additional memory space is allocated for every application of the "hfold". Furthermore, Viper understands congruence, so if two applications of "hfold" have the same arguments, they are considered equal.

\section{Well-definedness Checks}
There are four main well-definedness requirements that need to be checked for soundness.
\begin{enumerate}
    \item Types have to match
    \item Operator must be associative, commutative, and have an identity
    \item Receiver must be injective on the filter
    \item Current state must have enough permissions to all heap locations defined by the filter and receiver
\end{enumerate}

Correct typing is ensured by Viper's backend typechecker. The next chapter will discuss how we use the typechecker in our plugin. 

\subsection{Operator associativity, commutativity, and identity}
The operator's well-definedness checks can be done in a 
separate method. Putting these checks in a method means they are verified only once, avoiding unnecessary repeated checks.

For example, the plugin would generate the following method to check the "add()" operator. The first assert checks commutativity. The second assert checks associativity. The final assert checks that the identity is actually the identity. 
\begin{lstlisting}
method operator_add_welldef_check()
{
assert (forall _i1: Int, _i2: Int ::
  { (opApply((add()), _i1, _i2)) }
  (opApply((add()), _i1, _i2)) ==
  (opApply((add()), _i2, _i1)))
assert (forall _i1: Int, _i2: Int, _i3: Int ::
  { (opApply((opApply((add()), _i1, _i2)), _i3)) }
  (opApply((opApply((add()), _i1, _i2)), _i3)) ==
  (opApply(_i1, (opApply((add()), _i2, _i3)))))
assert (forall  _i1: Int ::
  { (opApply(_i1, (opGetIden((add()))))) }
  (opApply(_i1, (opGetIden((add()))))) == _i1)
}
\end{lstlisting}
Although these properties are important for soundness, we do not need them in any of the proofs. They do not need to appear as axioms, so the separate method checks are sufficient. Also, the triggers in the assertions are essentially mute because we do not intend the quantifiers to instantiate anywhere else. 

\subsection{Injectivity Of Receiver on the Filter}
Injectivity of the receiver with respect to the filter is required by quantified permissions. Since we use quantified permissions in the snapshot function specifications, Viper must be able to prove injectivity to avoid well-definedness errors in using the snapshots. 

In addition, we also rely on having an inverse in some of our strategies. For example, in the inlined axiom for singleton decomposition (explained in the upcoming sections), we use "recInv(loc(a,i))" to extract the index "i". This inverse would not exist without the injectivity of the receiver.

There are a few challenges and requirements for implementing the injectivity well-definedness checks.
\begin{itemize}
    \item Injectivity check should throw an error if it fails.
    \item Injectivity is required for quantified permissions, so an injectivity axiom must exist after the check. We cannot have an assertion in a separate method that we discard, unlike the techniques for checking operator well-definedness.
    \item We should avoid repeated injectivity checks. A receiver and filter pair should only be checked once.
\end{itemize}

By the first requirement, the injectivity check cannot be done in a domain axiom. There should be an assertion error if the checks fail, and domain axioms do not generate those errors. So, it must be some kind of assertion or precondition check. 

An alternative we previously considered to check injectivity is simply adding "assert" statements prior to user mentioned "fold" expressions. However, this technique would limit the usage of fold expressions outside a method body, where assert statements cannot be added in. For example, if a fold expression appears inside a heap-dependent function body, there is nowhere to add an assert statement because a function body can only have an expression! 

\subsubsection{Injectivity Check in Snapshot Precondition}
We chose to encode an injectivity check as a precondition to the snapshot function, as shown in the following code snippet.
\begin{lstlisting}
function snap_Int_Int_Int_val(fo: Fold[Int, Int, Int], 
        filter: Set[Int]): Map[Int,Int]
    requires forall ind1: Int, ind2: Int ::
      { ind1 in filter, ind2 in filter }
      ind1 in filter && ind2 in filter 
        && ind1 != ind2 ==>
      recApply((getreceiver(fo)),ind1) !=
        recApply((getreceiver(fo)),ind2)
        
    // the next precondition has quantified permissions
    requires forall i: Int :: {i in filter}  i in filter ==> 
        acc(recApply(getreceiver(fo), i).val)
\end{lstlisting}
The check is done according to the standard definition of injectivity. The receiver "r" is injective with respect to "filter" if 
$$\forall i, j \in "filter" \ldotp i \neq j \implies "r(i)" \neq  "r(j)". $$
Given two distinct indices in the filter, the receiver applied to each index should point to different references. 

Having this as a precondition ensures that before a snapshot is taken, we check for injectivity. Should the check fail, there will be a precondition check failure error. Notice that we place this precondition above the quantified permissions precondition; Quantified permissions in Viper require injectivity of the receiver, so having the precondition earlier means we prove it first.

\subsubsection{filterReceiverGood Flag}
We may also want to save this knowledge of injectivity for later, so we do not need to prove it again. We declare an additional domain function 
\begin{lstlisting}
    function filterReceiverGood(f: Set[A],r: Receiver[A])
\end{lstlisting}
to achieve this. If an instance of "filterReceiverGood(f,r)" is true then receiver "r" is injective over some filter "f". Once we prove injectivity fully with the precondition, the snap function can have a postcondition ensuring "filterReceiverGood(fo,r)."
\begin{lstlisting}
function snap_Int_Int_Int_val(fo: Fold[Int, Int, Int], 
        filter: Set[Int]): Map[Int,Int]
    ....
    ensures filterReceiverGood(filter, getreceiver(fo))
\end{lstlisting}

If we know "filterReceiverGood(f,r)", we should be able to skip the snapshot precondition check. To encode this skipping, we use a short-circuiting disjunction in the precondition with "filterReceiverGood(f,r)" as the first term:
\begin{lstlisting}
function snap_Int_Int_Int_val(fo: Fold[Int, Int, Int], 
        filter: Set[Int]): Map[Int,Int]
    requires filterReceiverGood(f, getreceiver(fo))  ||     
        forall ind1: Int, ind2: Int ::
      { ind1 in filter, ind2 in filter }
      ind1 in filter && ind2 in filter 
        && ind1 != ind2 ==>
      recApply((getreceiver(fo)),ind1) !=
        recApply((getreceiver(fo)),ind2).
\end{lstlisting}
This way, if a filter and receiver pair is already known to be injective through "filterReceiverGood(f, getreceiver(fo))", the disjunction evaluates to true without needing to verify the second term.

We can define additional axioms for "filterReceiverGood" in a domain declaration. Specifically, we add to the "Receiver" domain the axioms stating that injective receiver and filter pairing have an inverse. 
\begin{lstlisting}
axiom _inverse_receiver {
    (forall a: A, f: Set[A], r: Receiver[A] ::
      { (recApply(r, a)), (filterReceiverGood(f, r)) } { a in f, (filterReceiverGood(f, r)) }
      (filterReceiverGood(f, r)) && (a in f) ==>
        (recInv(r, (recApply(r, a)))) == a)
  }
  
axiom _inverse_receiver1 {
    (forall ref: Ref, f: Set[A], r: Receiver[A] ::
      { (filterReceiverGood(f, r)), (recInv(r, ref)) }
      (filterReceiverGood(f, r)) && ((recInv(r, ref)) in f) ==>
      (recApply(r, (recInv(r, ref)))) == ref)
  }
\end{lstlisting}
There are two inverse axioms. One states that given an element "a" in the filter, "recInv(r(a)) = a". The second axiom goes the other way; for some arbitrary reference, "ref," given that "recInv(ref)" is in the filter, then "r(recInv(ref)) = ref." The two distinct axioms are chosen for triggering completeness. Applying a "recApply" function explicitly would instantiate the first inverse axiom. Typically, in our receiver definitions, a receiver expression like "loc(a,i)" can be used as a trigger to instantiate the corresponding "recApply(r,i)",   (see our receiver definition for array sum before). However, there are some receiver expressions that are not valid triggers, such as the identity receiver from "Ref" to "Ref", so an explicit "recApply" may not be instantiated as a term. The second axiom ensures that if we apply a "recInv" function to that "Ref", such as in our inlining strategy, we still instantiate a "recApply" term and gain some knowledge about the inverse. More on triggers in the upcoming section.

We also generate other useful axioms that can infer further injectivity properties. For example, if a receiver "r" is injective over a filter "f", then it still is injective over a subset of "f". This is particularly useful when we decompose folds by taking subsets of the filter. Some other properties are
\begin{itemize}
    \item If receiver "r" is injective over a filter "f", then it is still injective over filter "f setminus f_1" for any filter "f_1"
    \item If receiver "r" is injective over a filter "f_1 union f_2", then it is still injective over filter "f_1" and over filter "f_2" separately. 
\end{itemize}

\subsubsection{Current state must have enough permissions to all heap locations defined by the filter and receiver}
The well-definedness condition of having permissions to all heap locations is checked automatically in the snapshot precondition. Recall that the snapshot precondition uses quantified permissions to check access to all heap locations defined by the receiver, filter, and field. This check will be done at the " fold " call site.

\section{Encoding the Modular Strategies}
Suppose the user writes an expression with a fold application denoting an array sum over all elements: 
\begin{lstlisting}
hfoldApply(hfold(arrayRec(a), mapIdentity(), add()),
    snap_Int_Int_Int_val(
        hfold(arrayRec(a), mapIdentity(), add()), 
        intRange(0, len(a)))).
\end{lstlisting}
We disregard the user-facing syntax for now, but the user will essentially write something similar that gets compiled down to that expression. For brevity, we write "fold(snap(intRange(0, len(a))))" as a short notation for the aforementioned fold application. This is analogous to the previous formal notation $\textrm{fold}_\sigma(\textrm{filter})$, with "snap" representing the snapshot of heap $\sigma$.  We will omit the receiver, mapping, and operator unless they are directly relevant. 

Recall that our strategies for reasoning about heap-dependent folds rely on the appropriate response to each heap-modifying operation. The final strategies for heap-modifying nodes are in the definition on \cpageref{strat:final}.
We will go through each one of these and discuss the axioms needed to encode these strategies. As we proceed, we will also discuss the trigger choices for each of the axioms

\subsection{Encoding the Reassignment Strategy}
Considering the following program where 2 elements of the array get incremented by 1 each, we want to prove that the fold representing the sum of the array has increased by 2.
\begin{lstlisting}
{
method arrayReassignment(a: Array, i: Int, j: Int) 
    requires // access to the entire array
    requires len(a) > 0
    requires i >= 0 && i < len(a)
    requires j >= 0 && j < len(a)
    ensures // access to the entire array
{
    loc(a,i).val := loc(a,i).val + 1
    label afterI

    loc(a,j).val := loc(a,j).val + 1
    label afterJ    
    
    assert fold(snap(intRange(0, len(a)))) == 
            old(fold(snap(intRange(0, len(a))))) + 2
}
\end{lstlisting}
We omit some of the quantified permissions for brevity.

Following the input and output fold model we previously discussed, an input fold to the first reassignment node is "old(fold(snap(intRange(0, len(a)))))". The output fold should then be the same, but in a different heap state, \ie, "old[afterI](fold(snap(intRange(0, len(a))))),"
Recall that the "old" keyword can take an optional label denoting the specific heap state. This generated output fold is considered an intermediate fold because the user only explicitly writes the fold expression in the heap state at the start and end of the method; now we have generated a fold in the state "afterI".

The second thing to do for a reassignment node is to perform a decomposition on both input and output folds by removing the index reassigned. In this case, it is index "i", or in practice "recInv(loc(a,i))."  Recall we had a domain axiom, especially for such a decomposition at \cref{axm:drpone}. That axiom has the trigger "triggerDeleteKey((hfoldApply(c, snap)), key)" where the first argument is the fold application and the second argument is the key to delete, "i" in our example. 

The decomposition generated will look like the following.
\begin{lstlisting}[label=decomsin1,caption=Singleton Decomposition Example]
hfoldApply(fo, snap) == 
  opApply(getoperator(fo), 
    hfoldApply1(fo, mapDelete(snap, Set(key))), 
    snap[key])
\end{lstlisting}
Previously, we explained that mapDelete deletes a set of keys from a map. In practice, this is implemented as a combination of a domain function with no axiom and a postcondition of the snapshot.
\begin{lstlisting}[label=axm:mapDelete,caption=Encoding of mapDelete]
function mapDelete(m: Map[A,B], e: Set[A]): Map[A,B] 
...
function snap_Int_Int_Int_val(fo: Fold[Int, Int, Int], filter: Set[Int]): Map[Int,Int]
    ...
    ensures  (forall s: Set[Int] ::
      { (mapDelete(result, s): Map[Int,Int]) }
      s != Set[Int]() ==>
      snap_Int_Int_Int_val_prime(fo, (filter setminus s)) ==
      (mapDelete(result, s): Map[Int,Int]))
\end{lstlisting}

Inside the snapshot function postcondition, we have an axiom which triggers on an application of "mapDelete" on the function output and some set "s". If the set is not empty, instantiate another snapshot with the same arguments except with "filter setminus s". Since the filter input to the snap function determines the keys available in the snapshot map, this essentially removes all keys that are part of "s". 

Another thing to note is that the precondition generates a \\ "snap_Int_Int_Int_val_prime," the prime version of the original snap function. We did this to prevent Viper's termination plugin from complaining and to stay consistent with the secondary folds. The folds generated in a decomposition are secondary folds annotated with a prime symbol or "1" in its Viper name. Similarly, we consider snapshots created from a decomposition to be "prime" versions. The full specifications of the "prime" snapshots are as follows.
\begin{lstlisting}[label=snapPrime,caption=Encoding of Snapshot Prime]
function snap_Int_Int_Int_val_prime(fo: Fold[Int, Int, Int], f: Set[Int]): Map[Int,Int]
  requires (filterReceiverGood(f, (getreceiver(fo))))
  requires (forall ind: Int ::
      { (ind in f) }
      (ind in f) ==>
      acc((recApply((getreceiver(fo)), ind)).val))
  ensures (filterReceiverGood(f, (getreceiver(fo))))
\end{lstlisting}
For snapshot primes, we remove most of the pre/post conditions of ordinary snapshots, leaving only the injectivity check via "filterReceiverGood" and the quantified permissions. The permissions used in the snap functions are important for Viper's function framing axioms, which we will use to prove the equality of folds across states. We discuss the relevance of function framing further in the upcoming paragraphs.

Technically, we could axiomatize mapDelete using domain axioms to handle general maps, but we found this to cause poor performance. Ultimately, we only use mapDelete with snapshots, so we do not need to generalize it. 

Going back to \cref{decomsin1}, applying the decomposition to the input fold "old(fold(snap(intRange(0, len(a)))))", we get
\begin{lstlisting}[label=singletoniIn,caption=Decomposition of Input Fold to Reassignment at index i]
    old(fold(snap(intRange(0, len(a))))) == 
    old(fold(snap(intRange(0, len(a)) setminus i))) +
    old(snap[i])".
\end{lstlisting}

Doing the same to the output fold "old[afterI](fold(snap(intRange(0, len(a)))))", we get
\begin{lstlisting}[label=singletoniOut,caption=Decomposition of Output Fold to Reassignment at index i]
    old[afterI](fold(snap(intRange(0, len(a))))) == 
    old[afterI](fold(snap(intRange(0, len(a)) setminus i))) +
    old[afterI](snap[i])".
\end{lstlisting}

Consider the 2nd term of the decomposition for both the input and output folds. the semantics of reassignment, Viper knows "old[afterI](snap[i]) = old(snap[i]) + 1." Then, to show that the fold has increased by 1, we need to prove that the 1st term of the decomposition for both input and output folds are equal; Viper derives this automatically from function framing.

Recall that the snap function requires permissions to all heap elements defined by the filter, receiver, and field. If at two program points, those heap elements stay the same, then the function output is the same. In our example method, we assume that the method holds permissions to the entire array, so between the start of the method and "label afterI", no other entity could have modified the array. Altogether, Viper can derive 
\begin{lstlisting}
    snap(intRange(0, len(a)) setminus i) == 
    old[afterI](snap(intRange(0, len(a)) setminus i))
\end{lstlisting}
because only the heap location at index "i" has been changed, and this snap instance does not need permissions to it. Keep in mind that the "old" keyword distributes inwards to the heap-dependent components in an expression. 
Because all its arguments are provably equal, 
\begin{lstlisting}
    old(fold(snap(intRange(0, len(a)) setminus i))) == 
    old[afterI](fold(snap(intRange(0, len(a)) setminus i))).
\end{lstlisting}

We can repeat the same exact steps for the reassignment to index "j", and the final assertion would verify.

\subsection{Inlined Axiom for Singleton Decomposition}
To implement this strategy, we inline an "assume" statement with an axiom at each relevant reassignment. For reassignments, the axiom is as follows. 
\begin{lstlisting}[label=inlSingleton,caption=Singleton Decomposition Axiom]
label previousLabel
loc(a,i).val := loc(a,i).val + 1
assume forall fo: Fold[Int, Int, Int], f: Set[Int] ::
  {old[previousLabel]((hfoldApply(fo, 
    snap_Int_Int_Int_val(fo, f)))) }
  filterReceiverGood(f, (getreceiver(fo))) &&
  (forall __ind: Int ::
    { (__ind in f) }
    (__ind in f) ==>
    perm((recApply((getreceiver(fo)), __ind)).val) ==
    write) ==>
  triggerDeleteKey(
    hfoldApply(fo, snap_Int_Int_Int_val(fo, f)),
    (recInv((getreceiver(fo)), loc(a,i)))) &&
  triggerDeleteKey(old[previousLabel](
    (hfoldApply(fo, snap_Int_Int_Int_val(fo, f)))), 
    (recInv((getreceiver(fo)), loc(a,i))))
\end{lstlisting}
The axiom quantifies on fold objects and filters. It triggers on an instance of a fold in the state directly before the reassignment, in our case. Note that the "old(A)" expression only works if there is a field access or a heap-dependent function in "A"; in our case, it works because of the snap function. If we instead quantified over maps, like "forall m: Map[Int,Int]," and replace the trigger with "m", the "old" expression will not work. This axiom would only instantiate for folds exactly in the state "previousLabel"; this is analogous to the local decomposition idea, which limits decomposition to fold in a specific state, local to the relevant heap-modifying node. 

For well-definedness, the LHS of the axiom requires injectivity of the receiver over the filter "f", as well as the correct permissions. Then, the RHS of the axiom simultaneously generates two fold applications and decompositions. The first "hfoldApply(fo, snap_Int_Int_Int_val(fo, f))" is the output intermediate fold, and the second is the input fold. Both of them get decomposed via the "triggerDeleteKey" function, and the appropriate key is obtained from the receiver inverse function "recInv."

An axiom should be generated for each combination of type variables in "Fold[A,V,B]" for existing fold instances.


\subsection{Encoding the Exhale Strategy}
Encoding the exhale strategy can be done in almost the same manner as the singleton strategy. If there is an exhale statement in the method that causes it to lose permissions, then we have to consider the input and output folds.

An output fold to an exhale node, as described in the definition on \cpageref{strat:final}, is $\textrm{fold}_{\sigma_j}(f')$ where $f'$ is the subset of $f$ where permissions were not lost. We previously described constructing a set of references that are lost in \cref{def:Lost} of \cpageref{def:Lost} and then using that to construct the subset of $f$ with permissions not lost in \cref{def:notLost}. Let us encode these definitions in Viper.

Let us consider the following working example.
\begin{lstlisting}[caption=Array Exhale Example]
{
method arrayExhale(a: Array, i: Int) 
    requires // access to the entire array
    requires len(a) > 0
    requires i >= 0 && i < len(a)
    ensures // access to the entire array
{
    label l0
    assume fold(snap(intRange(0, len(a)))) == 10 
    assume fold(snap(intRange(0, i))) == 4
    
    exhale forall e: Int:: i in intRange(0, i) ==> 
            acc(loc(a,i).val) 
    label l1

    assert fold(snap(intRange(i, len(a)))) == 6
}
\end{lstlisting}
To encode our exhale strategy on our example, first we need to declare a new variable "lostP_val_l1", denoting the references with permission lost to field "val".
\begin{lstlisting}
var lostP_val_l1: Set[Ref]
assume forall pElem: Ref ::
    { (pElem in lostP_val_l1) }
    (pElem in lostP_val_l1) ==
        (perm(pElem.val) == none &&
        old[l0](perm(pElem.val)) == write)
\end{lstlisting}
The axiom in the "assume" statement says that an element "pElem" is in this set if and only if the permission to "pElem.val" was previous full write access at state "l0" and the permissions is now "none" in the new state,\ie, the method lost permissions to this reference's "val" field. For every relevant field in the exhale, we generate one such variable and axiom pair.

Next, we want to construct the filter $f'$ of remaining elements from the original filter $f$, which is  $intRange(0, len(a))$ in the above example. To do so, we define a domain function "filterNotLost()" denoting the remaining filter after some permissions were lost, as in \cref{def:notLost}.
\begin{lstlisting}[label=axmNotLost, caption=Encoding Filter Remaining after Exhale]
function filterNotLost(f1: Set[A], r: Receiver[A], lostR: Set[Ref]): Set[A] 

axiom _filterNotLostAxiom {
(forall a: A, fs: Set[A], r: Receiver[A], lostR: Set[Ref] ::
  { a in filterNotLost(fs, r, lostR) }
  (a in filterNotLost(fs, r, lostR)) ==
  ((a in fs) && !((recApply(r, a) in lostR))))
}

axiom _filterNotLostSubset {
(forall fs: Set[A], r: Receiver[A], lostR: Set[Ref] ::
  { (filterNotLost(fs, r, lostR)) }
  (filterNotLost(fs, r, lostR)) subset fs)
}
\end{lstlisting}
The first axiom says element "a" is in the new filter if it is in the original filter, and "receiver(a)" is not a reference in the lost references set. The second axiom states explicitly that this new filter is a subset of the original filter. The trigger choices in these axioms are standard.

\subsection{Inlined Axiom for Exhales Decomposition}
With all of these sets constructed, we implement the exhale strategy by inlining another axiom after the exhale statement.
\begin{lstlisting}[label=strat:Exh, caption= Exhale Decomposition Axiom]
label l0
exhale forall e: Int:: i in intRange(0, i) ==> 
            acc(loc(a,i).val)
label l1
var lostP_val_l1: Set[Ref]
// omitted the axiom for lostP

assume forall fo: Fold[Int, Int, Int], f: Set[Int] ::
{ old[l0]((hfoldApply(fo, snap_Int_Int_Int_val(fo, f)))) }
(filterReceiverGood(f, (getreceiver(fo)))) &&
((forall __ind: Int ::
  { (__ind in f) }
  (__ind in f) ==>
  old[l0](perm((recApply((getreceiver(fo)),
  __ind): Ref).val) ==
  write)) &&
(forall __ind: Int ::
  { (__ind in
  (filterNotLost(f, (getreceiver(fo)), lostP_val_l1))) }
  (__ind in
  (filterNotLost(f, (getreceiver(fo)), lostP_val_l1))) ==>
  perm((recApply((getreceiver(fo)), __ind): Ref).val) ==
  write)) ==>
exhaleCompMap(fo, snap_Int_Int_Int_val(fo,
f), 0)
  
dummy1(hfoldApply(fo, snap_Int_Int_Int_val(fo, (filterNotLost(f,
(getreceiver(fo)), lostP_val_l1))))) &&
  
triggerDeleteBlock(old[l0]((hfoldApply(fo, snap_Int_Int_Int_val(fo,
fo)))), (filterNotLost(f, (getreceiver(fo)),
lostP_val_l1))) 
\end{lstlisting}
The LHS of the axiom encodes the well-definedness checks of the strategy. The checks require injectivity of the receiver on the filter, old state permissions to the heap locations prescribed by the filter, receiver, and field, and current state permissions to heap locations prescribed by the newly created "filterNotLost" filter.

The RHS of the axiom first generates an "exhaleCompMap" instance that takes the fold object, the old snapshot, and a fieldID integer as input. This will be used as a trigger in the inhale strategy, where we try to reconstruct a filter that existed before the exhale. Think of the "exhaleCompMap" as saving this fold and snapshot pair for the later inhale strategy axiom.

generates the output fold using a dummy function. "dummy1" simply generates the fold without giving any additional information about it; think of it as constructing the output fold with no extra assumption.
Then, the second conjunct uses a triggering function "triggerDeleteBlock" to decompose the input fold (the old one), as described by the listing on \cpageref{loseMany}. The decomposition looks something like this.
\begin{lstlisting}[label=decomsin1,caption=Singleton Decomposition Example]
hfoldApply(fo, snap) == 
  opApply(getoperator(fo), 
    hfoldApply1(fo, mapDelete(snap, keys)), 
    hfoldApply1(fo, mapSubmap(snap, keys)))
\end{lstlisting}
We have previously explained "mapDelete" as a function implemented by a post-condition of the snap function; "mapSubmap" is the same except the opposite. Instead of deleting keys from the map, it keeps only these keys on the map.

The following code snippet summarizes the encoding for "mapSubmap".
\begin{lstlisting}[label=axm:mapSubmap,caption=Encoding of mapSubmap]
function mapSubmap(m: Map[A,B], e: Set[A]): Map[A,B] 
...
function snap_Int_Int_Int_val(fo: Fold[Int, Int, Int], filter: Set[Int]): Map[Int,Int]
    ...
    ensures (forall s: Set[Int] ::
      { (mapSubmap(result, s): Map[Int,Int]) }
       (s subset filter) ==>
      snap_Int_Int_Int_val_prime(fo, s) ==
      (mapSubmap(result, s): Map[Int,Int]))
\end{lstlisting}
Again, the output is a "prime" instance of the snap function for the same reasons as the "mapDelete" axiomatization.

Altogether, this will decompose the input fold "fold(snap(intRange(0, len(a))))" as
\begin{lstlisting}
old(fold(snap(intRange(0, len(a))))) =
    old(fold(snap(intRange(0, i)))) + 
    old(fold(snap(intRange(i, len(a))))).
\end{lstlisting}
Technically, some of these terms will be in terms of "setminus" and "filterNotLost," but we have simplified that for readability. 

The output fold generated will be "fold(snap(intRange(i, len(a))))," which by Viper framing is equal to "old(fold(snap(intRange(i, len(a)))))." The assertion will go through because $6 + 4 = 10$.

\subsection{Encoding the Inhale Strategy }
Finally, we encode the inhale strategy as described in the definition on \cpageref{strat:final} and the list above the definition.  Starting from some input "fold(snap(f))", we want to construct an output fold "fold(snap(f*))" where "f*" is a bigger filter. We derive this bigger filter from an input fold to exhale nodes. Recall that we introduce the function "exhaleCompMap(fo, m, id)" for saving the input folds to exhale nodes and their snapshot "m". We will use this as a trigger in the inlined axiom for inhale.

\subsection{Inlined Axiom For Inhale Decomposition}
Our plugin inlines the following assume statement after a relevant inhale where permissions are gained. Suppose the state before the inhale has the label "l0", and the inhale node has input filter "fold(snap(f))."
\begin{lstlisting}
assume (forall fo: Fold[Int, Int, Int], f: Set[Int], m: Map[Int,Int] ::
  { old[l0]((hfoldApply(fo, snap_Int_Int_Int_val(fo, f)))),
  (exhaleCompMap(fo, m, 0)) }
  filterReceiverGood(f, (getreceiver(fo))) &&
  (filterReceiverGood(domain(m), (getreceiver(fo))) &&
  (f subset domain(m)) &&
  (forall __ind: Int ::
    { (__ind in domain(m)) }
    (__ind in domain(m)) ==>
    perm((recApply((getreceiver(fo)), __ind)).val) ==
    write)) ==>
  triggerDeleteBlock(
    (hfoldApply(fo, snap_Int_Int_Int_val(fo, domain(m)))),f) &&
  triggerDeleteBlock(
    (hfoldApply(fo, m)), f))
\end{lstlisting}
The trigger for this axiom is a fold application immediately before the inhale statement (in state "l0"), and "exhaleCompMap(fo, m, 0)," where 0 is the fieldID of field "val". Our plugin will keep track of these IDs internally, and each field will have a unique ID.  The "exhaleCompMap" will be generated for all folds and snapshots that are input folds to exhale nodes, and each instance of the function application will match the trigger. We obtain the big filter "f*" by taking the domain of map "m" as matched in the trigger "exhaleCompMap(fo, m, 0)." Recall that the domain of a snapshot is its filter argument.

The LHS of the axioms denotes the well-definedness checks, such as the injectivity of the receiver and the usual permissions. There are two separate injectivity conditions; the first is the injectivity of the input fold to the inhale. The second is the injectivity of the output fold, which we will generate. The third condition on the LHS "(f subset domain(m))" requires that the input fold's filter "f" is actually a subset of "f*". The final condition on the LHS uses quantified permissions to check that the method has permissions to the heap locations denoted by filter "f*".

On the RHS of the axiom, the first "triggerDeleteBlock" generates the output fold "fold(snap(f*))" and decomposes it by subtracting the filter "f" simultaneously. The second "triggerDeleteBlock" decomposes the exhale node's input fold in the same way for symmetry, as previously described.


% \subsection{Example: Putting it all Together}

% Consider the following example with exhales, reassignments, and inhales. We will describe how out strategies work to verify the assertions at the end. Assume that the appropriate inlining happens, all permissions are held at the start of the method, and the fold we consider is the array sum.  
% \begin{lstlisting}
% label l0
% exhale forall i: Int :: i in intRange(8,10) ==> 
%         acc(loc(a,i).val)
% label l1
% loc(a,0).val := loc(a,0).val + 1 
% label l2
% exhale forall i: Int :: i in intRange(4,8) ==> 
%         acc(loc(a,i).val)
% label l3
% loc(a,0).val := loc(a,0).val + 1 
% label l4
% inhale forall i: Int :: i in intRange(4,10) ==> 
%         acc(loc(a,i).val)
% label l5
%     // frame axiom

% inhale forall i: Int :: i in intRange(4,10) ==> 
%         loc(a,i).val == old(loc(a,i).val)

% // using short notation of fold
% assert fold[sum](snap(intRange(0,4))) == old(fold[sum](snap(intRange(0,4)))) + 2
% assert fold[sum](snap(intRange(0,10))) == old(fold[sum](snap(intRange(0,10)))) + 2
% \end{lstlisting}
% There are 5 heap-modifying nodes in this program (the last inhale does not modify state). At the very start, there are two different fold inputs: "old(fold[sum](snap(intRange(0,4))))" and "old(fold[sum](snap(intRange(0,10))))". Let us first consider the first one.

% The fold input "old(fold[sum](snap(intRange(0,4))))" enters the first exhale node, and generates the output fold "old[l1](fold(snap(intRange(0,4))))" with no decompositions, because the exhale did not lost any permissions to this fold's filter. The next reassignment node would 


\chapter{Implementation in Scala}
To do after the 14th of December. 
\section{User Syntax}

\section{Triggers Rationale?}

\section{Silver Plugin Architecture}

\section{Typechecking}

\section{Generating Inlined Axioms}

\chapter{Evaluation}
To do after the 14th of December. 

\section{Experiments with Tests}

\section{Challenges in Implementation}

\section{Limitations to Users}

\chapter{Other Lessons}
To do after the 14th of December. 

\section{Implication Support in SMT}

\section{Undocumented Behavior of Heap-dependent Triggers}

\section{Generated Axioms with no Triggers}

\chapter{Related Works}
	

\section{Spec\#}
Spec\# is a specification language on top of C\# developed by Microsoft Research. It provides predefined fold operations, such as sum, count, product, min, and max. In the original paper, these folds were referred to as ``comprehensions,'' which may explain why similar work has used the same term. However, our project uses the term "heap-dependent fold".

The folds in Spec\# are limited only to integer indices, whereas our plugin allows any arbitrary types. The axioms in Spec\# also assume some ordering in its support for decompositions. A fold can be decomposed by removing an element at the lowest index or at the highest index. This works for integer ranges but is not scalable to arbitrary types. This also means Spec\# does not support arbitrarily pulling out a random integer "i" in a decomposition, which we do. This is a crucial feature in our implementation because we want to support reasoning for random access data structures, which, by definition, can be accessed at any arbitrary point.

Spec\# does have an axiom for decomposing a fold into two folds with two smaller filters. However, this requires manual triggering from the user. Two folds must exist with one high index equal to the other low index; the two can then be added together. Our tool instead detects relevant filters according to heap modifications and generates the decomposition manually.    

Though the examples presented in the Spec\# paper all included arrays, technically, it also supports folds that are not heap-dependent. The fold can be on any arbitrary expression that may utilize an integer index. Our tool only supports heap-dependent folds, but we can also rely on heap modifications to pick the appropriate decompositions. Spec\# does not use heap reads or writes to axiomatize its folds.

\section{Tierry's Thesis}
Tierry Hoermann's bachelor’s thesis also attempted to implement folds (also called comprehensions, likely because of Spec\#) in Viper. His implementation was in Boogie, which allows him to write quantifiers on the heap. Instead, our plugin adds code to the original Viper file. 

One standout feature of his axioms is allowing arbitrary combinations of filters. In the general case, his axiom checks each filter against every other filter for subset or disjoint relations. If the subset relation holds, the two filters subtract to generate another filter. If they are disjoint, the axioms generate a new filter, which is the union of the two disjoint filters. Although the newly formed filters are prevented from generating more filters, similar to our secondary fold idea, the checks are still very costly. Each of these checks happens for each pair of filters, which leads to at least a polynomial complexity with respect to the number of filters.

Hoermann does perform automatic decomposition based on heap reassignments with the operator ":=." However, his analysis shows that the time complexity for singleton decomposition is $O(n^n)$, with $n$ as the number of heap locations modified. We have reduced this to linear by using our local and intermediate strategy, allowing for modular handling at each heap access. 

Unfortunately, his thesis only partially touched on implementation details, and the project code on Bitbucket has been removed. We are unable to reproduce his results.

\section{Arshavir Ter-Gabrielyan 's Dissertation}
Arshavir Ter-Gabrielyan formalizes folds, which he referred to as comprehensions, to help with reasoning about reachability in Viper. In doing so, he defines snapshot maps, which we have used extensively in our implementation.

The input to his fold is a set of references, in contrast to a filter and a receiver function in our implementation. This allows him to circumvent the injectivity problem because each reference in a set of references is unique, by property of a set. In practice, however, this makes defining the input set more complicated and unnatural. The user would often need to declare a set and write additional axioms using some base indices and a receiver anyway. Furthermore, this design does not align with the design for quantified permissions, which we use extensively. 

Furthermore, Ter-Gabrielyan's folds are not exclusively heap-dependent, allowing a more general body in the fold expressions. In an interesting example, Ter-Gabrielyan proposes nested folds to reason about matrices. However, he does not discuss the automatic decomposition of the folds in response to heap updates. It is unclear how to reconcile heap-dependent components of the fold and non-heap-dependent components, especially when certain components may interact with each other. 

\chapter{Conclusion and Future Work}
We have designed a plugin for supporting heap-dependent folds in Viper. This allows the user to reason about properties of random access data structures without relying on induction. Folds are decomposed automatically in response to heap reads and updates, namely reassignment, exhales, and inhales. 

In addition, we introduce a modular strategy to reason about each heap change locally, reducing coupling between folds at different states. We solved the combinatorial explosion resulting from repeated decomposition and reduced the number of generated folds to $O(n)$, with $n$ as the number of heap updates. We implemented all this as a plugin in Viper, only modifying the Viper input. This should make the plugin compatible with any frontend and verifier, though only the Carbon verifier works with it in practice.


\section{Future Work}
\subsection{Implementation in the Verifier}
Further research could attempt to implement fold supports at the verifier level instead of the Viper level. More fine-grain control of the calls to the SMT solver could lead to more flexibility. It may be possible to remove case splits that have high-performance costs.

Furthermore, we relied heavily on interactions with the heap, which could be done more explicitly in Boogie. Implementing folds using the Carbon verifier, which translates Viper into Boogie, could make the heap-dependent components of the fold easier to handle. 

\subsection{Heap-dependent Triggers}
Our axioms rely heavily on heap-dependent triggers despite the lack of research into their behaviour. From our own experiments, much of the undocumented behaviour is unexpected, \ie, some axioms trigger when they are not supposed to, or some axioms do not trigger when they should. For example, triggers that mention a later state never instantiate the axiom. Further research into state-dependent triggers and refinements to the Boogie encoding of the heap-dependent functions may help. 


\subsection{Optimization}
Our experiments showed some surprisingly poor performance, but we lack the tools to investigate the causes of these performance drops properly. Though the Axiom Profiler exists, we are still missing tools to visualize the proofs for individual assertions. Research into better debugging tools could help us examine the relevant axioms and instances for each assertion; this way, we can also isolate the performance hogs and remove them if possible.

\subsection{Syntax Improvements}
With our plugin, a component of a fold has to be individually declared explicitly at the top level. Ideally, a fold can be defined with inlined anonymous functions, as commonly found in general programming languages. Further research could extend the techniques of defunctionalization to support inlined function declarations, which could be useful to Viper overall, not just for expressing folds.  

\subsection{Support for General Folds}
Our plugin supports heap-dependent folds, not arbitrary folds. It cannot express a sum of an arbitrary set of integers. Further research could examine the axiomatizations to support general folds. In particular, how to reconcile the techniques for non-heap-dependent and heap-dependent folds is unclear. If a fold contains both heap-dependent and independent portions, how should the decompositions look in response to heap reads, inhales, exhales, and reassignment? 

\chapter{LATEX STUFF}

Here is a quote:
\begin{quote}
  % It is centered
  \begin{center}
    This is a small poem,\\
    a little poem, a Haiku,\\
    to show you how to.\\
    ---Michael M$^{\rm c}$Neil Forbes.
  \end{center}
\end{quote}

This small poem shows several features:
\begin{itemize}
\item The use of the \verb|quote| and \verb|center| environments.
\item The \verb|\newpage| command has been used to force a page
  break.  (Sections do not usually start on a new page.)
\item The pagestyle has been set to suppress the headers using the
  command \verb|\thispagestyle{plain}|.  Note that using
  \verb|\pagestyle{plain}| would have affected all of the subsequent
  pages.
\end{itemize}
\section{Programs}
Here we give an example of a new float as defined using the
\texttt{float} package.  In the preamble we have used the commands
\begin{verbatim}
\floatstyle{ruled}
\newfloat{Program}{htbp}{lop}[chapter]
\end{verbatim}
This creates a ``Program'' environment that may be used for program
fragments.  A sample \texttt{python} program is shown in
Program~\ref{prog:fib}.  (Note that Python places a fairly restrictive
limit on recursion so trying to call this with a large $n$ before
building up the cache is likely to fail unless you increase the
recursion depth.)
\begin{Program}
  \caption{\label{prog:fib} Python program that computes the $n^{\rm
      th}$ Fibonacci number using memoization.}
\begin{verbatim}
def fib(n,_cache={}):
    if n < 2:
        return 1
    if n in _cache:
        return _cache[n]
    else:
        result = fib(n-1)+fib(n-2)
        _cache[n] = result
        return result
\end{verbatim}
\end{Program}
Instead of using a \texttt{verbatim} environment for your program
chunks, you might like to \texttt{include} them within an
\texttt{alltt} envrironment by including the \verb|\usepackage{alltt}|
package (see page 187 of the \LaTeX{} book).  Another useful package
is the \verb|\usepackage{listings}| which can pretty-print many
different types of source code.

% Force a new page
\newpage

%% Here we provide a short optional argument to \chapter[]{}.  This
%% optional argument will appear in the table of contents.  For long
%% titles, one should use this to give a single-line entry to the
%% table of contents.
\chapter[Another Chapter\ldots]{Another Chapter with a Very Long
  Chapter-name that will Probably Cause Problems}
\label{cha:apple_ref}

This chapter name is very long and does not display properly in the
running headers or in the table of contents.  To deal with this, we
provide a shorter version of the title as the optional argument to the
\verb|\chapter[]{}| command.

For example, this chapter's title and associated table of contents heading and
running header was created with\\
\verb|\chapter[Another Chapter\ldots]{Another Chapter with a Very Long|\\
\verb|Chapter-name that will Probably Cause Problems}|.

Note that, according to the thesis regulations, the heading included
in the table of contents must be a truncation of the actual heading.

This Chapter was used as a demonstration in the Preface for how to
attribute contribution from collaborators.  If there are any such
contributions, details must be included in the Preface.  If you wish,
you may additionally use a footnote such as this.\footnote{This
  chapter is based on work conducted in UBC's Maple Syrup Laboratory
  by Dr. A. Apple, Professor B. Boat, and C. Cat.}

\section{Another Section}
Another bunch of text to demonstrate what this file does.
You might want a list for example:\footnote{Here is a footnote in a
  different chapter.  Footnotes should come after punctuation.}
\begin{itemize}
\item An item in a list.
\item Another item in a list.
\end{itemize}

\section*{An Unnumbered Section That is Not Included in the Table of
  Contents}
\begin{figure}[ht]
  \begin{center}
%% psfrag: comment the following line if not using the psfrag package
    \psfrag{pie makes me happy!}{$\pi$ makes me happy!}
%% includegraphics: comment the following if not using the graphicx package
    \includegraphics[width=0.4\textwidth]{fig}
    \caption[Happy Face: figure example.]{\label{fig:happy} This is a figure of
      a happy face with a \texttt{psfrag} replacement.  The original figure
      (drawn in xfig and exported to a .eps file) has the text ``pie makes me
      happy!''.  The \texttt{psfrag} package replaces this with ``$\pi$ makes me
      happy!''.  Note: the Makefile compiles the sample using pdf\LaTeX\ which
      cannot use \texttt{psfrag} directly.  For some options that work with
      pdf\LaTeX, please see this discussion:
      \url{http://tex.stackexchange.com/questions/11839}.  For the caption, we
      have used the optional argument for the caption command so that only a
      short version of this caption occurs in the list of figures.}
  \end{center}
\end{figure}
\afterpage{\clearpage}
Here is an example of a figure environment.
Perhaps I should say that the example of a figure can be seen in
Figure~\ref{fig: happy}.  Figure placement can be tricky with \LaTeX\
because figures and tables are treated as ``floats'': text can flow
around them, but if there is not enough space, they will appear later.
To prevent figures from going too far, the
\verb|\afterpage{\clearpage}| command can be used.  This makes sure
that the figures are typesetted at the end of the page (possibly appearing on
their own on the following pages) and before any subsequent text.

The \verb|\clearpage| forces a page break so that the figure can be
placed, but without the the \verb|\afterpage{}| command, the page
would be broken too early (at the \verb|\clearpage| statement).  The
\verb|\afterpage{}| command tells \LaTeX{} to issue the command after
the present page has been rendered.

\section{Tables}
We have already included one table:~\ref {tab: Table1}.  Another table
is plopped right here.
\begin{table}[ht]
  \begin{center}
    \begin{tabular}{|l||l|l||l|l|}
      \hline
      &\multicolumn{2}{l|}{Singular}&\multicolumn{2}{l|}{Plural}\\
      \cline{2-5}
       &English&\textbf{Gaeilge}&English&\textbf{Gaeilge}\\
      \hline\hline
      1st Person&at me&\textbf{agam}&at us&\textbf{againn}\\
      2nd Person&at you&\textbf{agat}&at you&\textbf{agaibh}\\
      3rd Person&at him&\textbf{aige}&at them&\textbf{acu}\\
       &at her&\textbf{aici}& & \\
      \hline
    \end{tabular}
    \caption{
      \label{tab:Table2}
      Another table.}
  \end{center}
\end{table}
Well, actually, as with Figures, tables do not
necessarily appear right ``here'' because tables are also ``floats''.
\LaTeX{} puts them where it can.  Because of this, one should refer to
floats by their labels rather than by their location.  This example is
demonstrated by Table~\ref{tab:Table2}.  This one is pretty close,
however.  (Note: you should generally not put tables or figures in the
middle of a paragraph.  This example is for demonstration purposes
only.)

Another useful package is \verb|\usepackage{longtable}| which provides
the \texttt{longtable} environment.  This is nice because it allows
tables to span multiple pages.  Table~\ref{tab:longtable} has been
formatted this way.
\begin{center}
  \begin{longtable}{|l|l|l|}
    \caption{\label{tab:longtable}Feasible triples for
      highly variable Grid}\\

    \hline \multicolumn{1}{|c|}{\textbf{Time (s)}} &
    \multicolumn{1}{c|}{\textbf{Triple chosen}} &
    \multicolumn{1}{c|}{\textbf{Other feasible triples}} \\ \hline
    \endfirsthead

    \multicolumn{3}{c}%
    {{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
    \hline \multicolumn{1}{|c|}{\textbf{Time (s)}} &
    \multicolumn{1}{c|}{\textbf{Triple chosen}} &
    \multicolumn{1}{c|}{\textbf{Other feasible triples}} \\ \hline
    \endhead

    \hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline
    \endfoot

    \hline \hline
    \endlastfoot

    0 & (1, 11, 13725) & (1, 12, 10980), (1, 13, 8235), (2, 2, 0), (3, 1, 0) \\
    274 & (1, 12, 10980) & (1, 13, 8235), (2, 2, 0), (2, 3, 0), (3, 1, 0) \\
    5490 & (1, 12, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    8235 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    10980 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    13725 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    16470 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    19215 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    21960 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    24705 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    27450 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    30195 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    32940 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    35685 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    38430 & (1, 13, 10980) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    41175 & (1, 12, 13725) & (1, 13, 10980), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    43920 & (1, 13, 10980) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    46665 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    49410 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    52155 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    54900 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    57645 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    60390 & (1, 12, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    63135 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    65880 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    68625 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    71370 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    74115 & (1, 12, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    76860 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    79605 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    82350 & (1, 12, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    85095 & (1, 12, 13725) & (1, 13, 10980), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    87840 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    90585 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    93330 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    96075 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    98820 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    101565 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    104310 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    107055 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    109800 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    112545 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    115290 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    118035 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    120780 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    123525 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    126270 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    129015 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    131760 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    134505 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    137250 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    139995 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    142740 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    145485 & (1, 12, 16470) & (1, 13, 13725), (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    148230 & (2, 2, 2745) & (2, 3, 0), (3, 1, 0) \\
    150975 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    153720 & (1, 12, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    156465 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    159210 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    161955 & (1, 13, 16470) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
    164700 & (1, 13, 13725) & (2, 2, 2745), (2, 3, 0), (3, 1, 0) \\
\end{longtable}
\end{center}

\subsection*{An Unnumbered Subsection}
Note that if you use subsections or further divisions under an
unnumbered section, then you should make them unnumbered as well
otherwise, you will end up with zeros in the section numbering.

\chapter{Landscape Mode}
The landscape mode allows you to rotate a page through 90 degrees.  It
is generally not a good idea to make the chapter heading landscape,
but it can be useful for long tables, etc.

\begin{landscape}
  This text should appear rotated, allowing for formatting of very
  wide tables, etc.  Note that this might only work after you convert
  the \texttt{dvi} file to a postscript (\texttt{ps}) or \texttt{pdf}
  file using \texttt{dvips} or \texttt{dvipdf} etc.  This feature is
  provided by the \verb|lscape| and the \verb|pdflscape| packages.
  The latter is preferred if it works, as it also rotates the pages in
  the PDF file for easier viewing.
\end{landscape}

%% This file is set to use a BibTeX file sample.bib and uses the
%% plain style.  Other styles may be used depending on the conventions
%% of your field of study.
%%
%%% Note: the bibliography must come before the appendices.
\bibliographystyle{plain}
\bibliography{sample}

%% Use this to reset the appendix counter.  Note that the FoGS
%% requires that the word ``Appendices'' appear in the table of
%% contents either before each appendix label or as a division
%% denoting the start of the appendices.  We take the latter option
%% here.  This is ensured by making the \texttt{appendicestoc} option
%% a default option to the UBC thesis class.

%%% If you only have one appendix, please uncomment the following line.
% \renewcommand{\appendicesname}{Appendix}
\appendix
\chapter{First Appendix}
Here you can have your appendices.  Note that if you only have a
single appendix, you should issue
\verb|\renewcommand{\appendicesname}{Appendix}| before calling
\verb|\appendix| to display the singular ``Appendix'' rather than the
default plural ``Appendices''.

\chapter{Second Appendix}
Here is the second appendix.

%% This changes the headings and chapter titles (no numbers for
%% example).
\backmatter

%% Indices come here if you have them.

\chapter*{Additional Information}
This chapter shows you how to include additional information in your
thesis, the removal of which will not affect the submission.  Such
material should be removed before the thesis is actually submitted.

First, the chapter is unnumbered and not included in the Table of
Contents.  Second, it is the last section of the thesis, so its
removal will not alter any of the page numbering etc. for the previous
sections.  Do not include any floats, however, as these will appear in
the initial lists.

The \texttt{ubcthesis} \LaTeX{} class has been designed to aid you in
producing a thesis that conforms to the requirements of The
University of British Columbia Faculty of Graduate Studies (FoGS).

Proper use of this class and sample is highly recommended---and should
produce a well formatted document that meets the FoGS requirement.
Notwithstanding, complex theses may require additional formatting that
may conflict with some of the requirements.  We therefore \emph{highly
  recommend} that you consult one of the FoGS staff for assistance and
an assessment of potential problems \emph{before} starting final
draft.

While we have attemped to address most of the thesis formatting
requirements in these files, they do not constitute an official set of
thesis requirements.  The official requirements are available at the
following section of the FoGS web site:
\begin{center}
  \begin{tabular}{|l|}
    \hline
    \url{http://www.grad.ubc.ca/current-students/dissertation-thesis-preparation}\\
    \hline
  \end{tabular}
\end{center}
We recommend that you review these instructions carefully.

\end{document}
\endinput
%%
%% End of file `ubcsample.tex'.
